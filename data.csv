,user_input,reference_contexts,reference,synthesizer_name
0,How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?,"['1a80dd80-6c8b-4e15-b6c1-91003fa598fa\n\n2 Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.']","The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.",single_hop_specifc_query_synthesizer
1,What is the RAG-Sequnce model and how does it work?,"['c40ad8c2-1af8-4a6b-a3bf-b722a195985b\n\n2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) ≈ (cid:88) pη(z|x)pθ(y|x, z) = (cid:88) pη(z|x) N (cid:89) pθ(yi|x, z, y1:i−1) z∈top-k(p(·|x)) z∈top-k(p(·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token(y|x) ≈ N (cid:89) (cid:88) pη(z|x)pθ(yi|x, z, y1:i−1) i z∈top-k(p(·|x)) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j − log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To decode, we can plug p(cid:48) θ(yi|x, y1:i−1) = (cid:80) θ(yi|x, y1:i−1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”']","The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) ≈ (cid:88) pη(z|x)pθ(y|x, z) = (cid:88) pη(z|x) N (cid:89) pθ(yi|x, z, y1:i−1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.",single_hop_specifc_query_synthesizer
2,"As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?","['378ac7e6-9469-44a7-8134-3be9a8ad5219\n\n3 Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriﬁcation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.']","In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.",single_hop_specifc_query_synthesizer
3,Wht is the perfomance of T5 in open-domain QA tasks?,"['521b14e6-7228-4246-8929-6b41ae350481\n\n4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of ""open-book"" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding ""The Sun. BART completes the generation ""The Sun Also Rises"" is a novel by this author of ""The Sun Also Rises"" indicating the title ""The Sun Also Rises"" is stored in BART’s parameters. Similarly, BART will complete the partial decoding ""The Sun Also Rises"" is a novel by this author of ""A with ""The Sun Also Rises"" is a novel by this author of ""A Farewell to Arms"". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by ” this well a ” R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovel”AFarewelltoArms”(1929)...Document2:...artistsofthe1920s”LostGeneration”expatriatecommunity.Hisdebutnovel,”TheSunAlsoRises”,waspublishedin1926. is Fare BOS ises Doc3 ” Sun ” Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem- ingway"" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms"" and for document 2 when generating “The Sun Also Rises"". Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deﬁne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed']","On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.",single_hop_specifc_query_synthesizer
4,how samurai help in visual tracking?,"['48cdab7e-a2c8-4a01-9dc6-b72a7ba9adb1\n\n4 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639b8\n\n2. Related Works 2.1. Visual Object Tracking (VOT) Visual Object Tracking (VOT) [36] aims to track objects in challenging video sequences that include variations in ob- ject scale, occlusions, and complex backgrounds so as to elevate the robustness and accuracy of tracking algorithms. Siamese-based [10, 52] and transformer-based [12, 47] trackers are common approaches by learning embedding similarity. However, due to lacking self-correction of these trackers in the single forward pass evaluation scheme, they can easily drift toward distractors. To this end, recent works [18, 49] further introduce memory bank and attention to find a better mapping between current frame and history information. 2.2. Segment Anything Model (SAM) The Segment Anything Model (SAM) [26] has sparked con- siderable follow-up research since its introduction. SAM introduces a prompt-based segmentation approach, where users could input points, bounding boxes, or text to guide the model in segmenting any object within an image. The use of SAM has wide-ranging applications like in video un- derstanding [7, 38, 39] and editing [6]. Since then, various works have built upon SAM. For example, SAM 2 [35] ex- pands the model’s capabilities to video segmentation [11], incorporating memory mechanisms for tracking objects across multiple frames in dynamic video sequences. Ad- ditionally, efforts have been made to create more efficient variants of SAM for resource-constrained environments, aiming to reduce its computational demands [45, 54]. Re- search in medical imaging has also adopted SAM for spe- cialized tasks [30]. Recently, SAM2Long [14] uses tree- based memory to enhance object segmentation for long video. However, their higher FPS video sequences and deeper memory tree architectures require exponentially more computing power and memory storage due to the over- head of storing exact paths and time-constrained memory paths. On the other hand, our proposed SAMURAI model, which is built upon SAM 2, has been trained on large-scale segmentation datasets to overcome these challenges and en- sure good performance and generalization ability. 2.3. Motion Modeling Motion modeling is an important component in tracking tasks, which can be categorized into heuristic and learn- able approaches. Heuristic methods, such as the widely- used Kalman Filter (KF) [24], rely on fixed motion pri- ors and predefined hyper-parameters to predict object tra- jectories. While KF has proven effective in many track- ing benchmarks, it often fails in scenarios with intense or abrupt motion. Other methods [1] attempt to counteract intense or abrupt object motion by compensating for cam- era movement before applying traditional KF-based predic- 2 tion. However, both the standard and noise scale adaptive (NSA) Kalman Filters [15] come with a multitude of hyper- parameters, potentially restricting their effectiveness to spe- cific types of motion scenarios. In contrast, learnable mo- tion models have attracted increasing interest due to their data-driven nature. Tracktor [2] is the first to use trajec- tory boxes as Regions of Interest (RoI) in a Faster-RCNN to extract features and regress the object’s position across frames. MotionTrack [43] enhances tracking by learning past trajectory representations to predict future movements. MambaTrack [22] further explores different learning-based motion models architecture like transformer [40] and state- space model (SSM) [21]. Our approach is also a learning- based motion modeling with an enhanced heuristic scheme.', '6d638fba-501e-49fb-b7fe-f12c7cfc5635\n\n4. Method SAM 2 has demonstrated strong performance in basic Vi- sual Object Tracking (VOT) and Video Object Segmenta- tion (VOS) tasks. However, the original model can mistak- enly encode incorrect or low-confidence objects, leading to substantial error propagation in long-sequence VOT. To address the above issues, we propose a Kalman Fil- ter (KF)-based motion modeling on top of the multi-masks selection (in 4.1) and an enhanced memory selection based on a hybrid scoring system that combines affinity and mo- tion scores (in 4.2). These enhancements are designed to (2) (3) Figure 2. The overview of our SAMURAI visual object tracker. strengthen the model’s ability to track objects accurately in complex video scenarios. Importantly, this approach does not require fine-tuning, nor does it require additional train- ing, and it can be integrated directly into the existing SAM 2 model. By improving the selection of predicted masks with- out additional computational overhead, this method pro- vides a reliable, real-time solution for online VOT. M and the bounding box derived from the Kalman filter’s predicted state. We then select the mask that maximizes a weighted sum of the KF-IoU score and the original affinity score: M∗ = arg max (αkf · skf (Mi) + (1 − αkf ) · smask(Mi)). Mi 4.1. Motion Modeling Finally, the update is performed using: Motion modeling has long been an effective approach to Visual Object Tracking (VOT) and Multiple Object Track- ing (MOT) [1, 5, 51] in resolving association ambiguities. We employ the linear-based Kalman filter [24] as our base- line to demonstrate the incorporation of motion modeling in improving tracking accuracy. In our visual object tracking framework, we integrate the Kalman filter to enhance bounding box position and dimen- sion predictions, which in turn helps select the most confi- dent mask out of N candidates from M. We define the state vector x as: ˆxt|t = ˆxt|t−1 + Kt(zt − H ˆxt|t−1) where zt is the measurement, the bounding box derived from the mask we selected, used to update. F is the linear state transition matrix, Kn is the Kalman gain, and H is the observation matrix. Furthermore, to ensure the robustness of the motion modeling after the targeted object reappears or the poor mask qualities for a certain period of time, we also maintain a stable motion state where we take consider- ation of the motion module if and only if the tracked object is being successfully update in the past τkf frames. x = [x, y, w, h, ˙x, ˙y, ˙w, ˙h]T where x, y represents the center coordinate of the bound- ing box, w and h denote its width and height, respectively, and their corresponding velocities are represented by the dot notation. For each mask Mi, the corresponding bounding box di is derived by computing the minimum and maxi- mum x and y coordinates of the mask’s non-zero pixels. The Kalman filter operates in a predict-correct cycle, where the state prediction ˆxt+1|t is given by: (4)', '521b14e6-7228-4246-8929-6b41ae350481\n\n4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of ""open-book"" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding ""The Sun. BART completes the generation ""The Sun Also Rises"" is a novel by this author of ""The Sun Also Rises"" indicating the title ""The Sun Also Rises"" is stored in BART’s parameters. Similarly, BART will complete the partial decoding ""The Sun Also Rises"" is a novel by this author of ""A with ""The Sun Also Rises"" is a novel by this author of ""A Farewell to Arms"". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by ” this well a ” R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovel”AFarewelltoArms”(1929)...Document2:...artistsofthe1920s”LostGeneration”expatriatecommunity.Hisdebutnovel,”TheSunAlsoRises”,waspublishedin1926. is Fare BOS ises Doc3 ” Sun ” Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem- ingway"" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms"" and for document 2 when generating “The Sun Also Rises"". Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deﬁne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed', '81a7d139-e415-4c40-8254-27bb6dc39105\n\nan interactive demo of a RAG model can be found at https://huggingface.co/rag/ 2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers 17 D Further Details on Open-Domain QA For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur in top 1000 documents for the query. CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres- sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace. TriviaQA Evaluation setups The open-domain QA community customarily uses public develop- ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being simpler to answer from Wikipedia. E Further Details on FEVER For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The ﬁrst is to classify the claim as either ""Supported"", ""Refuted"" or ""Not Enough Info"", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work. F Null Document Probabilities We experimented with adding ""Null document"" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally ""retrieve"" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document mechanisms may not be necessary for RAG. G Parameters Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable 18 Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation Task Train Development Test Natural Questions TriviaQA WebQuestions CuratedTrec Jeopardy Question Generation MS-MARCO FEVER-3-way FEVER-2-way 79169 78786 3418 635 97392 153726 145450 96966 8758 8838 362 134 13714 12468 10000 6666 3611 11314 2033 635 26849 101093* 10000 6666 parameters. The best performing ""closed-book"" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating point precision to manage memory and disk footprints. H Retrieval Collapse In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks. I Number of instances per dataset The number of training, development and test datapoints in each of our datasets is shown in Table 7. 19']","SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.",multi_hop_abstract_query_synthesizer
5,"What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?","['48cdab7e-a2c8-4a01-9dc6-b72a7ba9adb1\n\n4 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639b8\n\n2. Related Works 2.1. Visual Object Tracking (VOT) Visual Object Tracking (VOT) [36] aims to track objects in challenging video sequences that include variations in ob- ject scale, occlusions, and complex backgrounds so as to elevate the robustness and accuracy of tracking algorithms. Siamese-based [10, 52] and transformer-based [12, 47] trackers are common approaches by learning embedding similarity. However, due to lacking self-correction of these trackers in the single forward pass evaluation scheme, they can easily drift toward distractors. To this end, recent works [18, 49] further introduce memory bank and attention to find a better mapping between current frame and history information. 2.2. Segment Anything Model (SAM) The Segment Anything Model (SAM) [26] has sparked con- siderable follow-up research since its introduction. SAM introduces a prompt-based segmentation approach, where users could input points, bounding boxes, or text to guide the model in segmenting any object within an image. The use of SAM has wide-ranging applications like in video un- derstanding [7, 38, 39] and editing [6]. Since then, various works have built upon SAM. For example, SAM 2 [35] ex- pands the model’s capabilities to video segmentation [11], incorporating memory mechanisms for tracking objects across multiple frames in dynamic video sequences. Ad- ditionally, efforts have been made to create more efficient variants of SAM for resource-constrained environments, aiming to reduce its computational demands [45, 54]. Re- search in medical imaging has also adopted SAM for spe- cialized tasks [30]. Recently, SAM2Long [14] uses tree- based memory to enhance object segmentation for long video. However, their higher FPS video sequences and deeper memory tree architectures require exponentially more computing power and memory storage due to the over- head of storing exact paths and time-constrained memory paths. On the other hand, our proposed SAMURAI model, which is built upon SAM 2, has been trained on large-scale segmentation datasets to overcome these challenges and en- sure good performance and generalization ability. 2.3. Motion Modeling Motion modeling is an important component in tracking tasks, which can be categorized into heuristic and learn- able approaches. Heuristic methods, such as the widely- used Kalman Filter (KF) [24], rely on fixed motion pri- ors and predefined hyper-parameters to predict object tra- jectories. While KF has proven effective in many track- ing benchmarks, it often fails in scenarios with intense or abrupt motion. Other methods [1] attempt to counteract intense or abrupt object motion by compensating for cam- era movement before applying traditional KF-based predic- 2 tion. However, both the standard and noise scale adaptive (NSA) Kalman Filters [15] come with a multitude of hyper- parameters, potentially restricting their effectiveness to spe- cific types of motion scenarios. In contrast, learnable mo- tion models have attracted increasing interest due to their data-driven nature. Tracktor [2] is the first to use trajec- tory boxes as Regions of Interest (RoI) in a Faster-RCNN to extract features and regress the object’s position across frames. MotionTrack [43] enhances tracking by learning past trajectory representations to predict future movements. MambaTrack [22] further explores different learning-based motion models architecture like transformer [40] and state- space model (SSM) [21]. Our approach is also a learning- based motion modeling with an enhanced heuristic scheme.', '6d638fba-501e-49fb-b7fe-f12c7cfc5635\n\n4. Method SAM 2 has demonstrated strong performance in basic Vi- sual Object Tracking (VOT) and Video Object Segmenta- tion (VOS) tasks. However, the original model can mistak- enly encode incorrect or low-confidence objects, leading to substantial error propagation in long-sequence VOT. To address the above issues, we propose a Kalman Fil- ter (KF)-based motion modeling on top of the multi-masks selection (in 4.1) and an enhanced memory selection based on a hybrid scoring system that combines affinity and mo- tion scores (in 4.2). These enhancements are designed to (2) (3) Figure 2. The overview of our SAMURAI visual object tracker. strengthen the model’s ability to track objects accurately in complex video scenarios. Importantly, this approach does not require fine-tuning, nor does it require additional train- ing, and it can be integrated directly into the existing SAM 2 model. By improving the selection of predicted masks with- out additional computational overhead, this method pro- vides a reliable, real-time solution for online VOT. M and the bounding box derived from the Kalman filter’s predicted state. We then select the mask that maximizes a weighted sum of the KF-IoU score and the original affinity score: M∗ = arg max (αkf · skf (Mi) + (1 − αkf ) · smask(Mi)). Mi 4.1. Motion Modeling Finally, the update is performed using: Motion modeling has long been an effective approach to Visual Object Tracking (VOT) and Multiple Object Track- ing (MOT) [1, 5, 51] in resolving association ambiguities. We employ the linear-based Kalman filter [24] as our base- line to demonstrate the incorporation of motion modeling in improving tracking accuracy. In our visual object tracking framework, we integrate the Kalman filter to enhance bounding box position and dimen- sion predictions, which in turn helps select the most confi- dent mask out of N candidates from M. We define the state vector x as: ˆxt|t = ˆxt|t−1 + Kt(zt − H ˆxt|t−1) where zt is the measurement, the bounding box derived from the mask we selected, used to update. F is the linear state transition matrix, Kn is the Kalman gain, and H is the observation matrix. Furthermore, to ensure the robustness of the motion modeling after the targeted object reappears or the poor mask qualities for a certain period of time, we also maintain a stable motion state where we take consider- ation of the motion module if and only if the tracked object is being successfully update in the past τkf frames. x = [x, y, w, h, ˙x, ˙y, ˙w, ˙h]T where x, y represents the center coordinate of the bound- ing box, w and h denote its width and height, respectively, and their corresponding velocities are represented by the dot notation. For each mask Mi, the corresponding bounding box di is derived by computing the minimum and maxi- mum x and y coordinates of the mask’s non-zero pixels. The Kalman filter operates in a predict-correct cycle, where the state prediction ˆxt+1|t is given by: (4)', '702254b9-86d6-469b-9ebd-fce417b63c5f\n\nI. INTRODUCTION H YPERSPECTRAL imaging (HSI) is a powerful remote in- sensing technique that captures detailed spectral formation across narrow wavelength bands, enabling precise identification and classification of diverse land-cover materials [1], [2]. Due to its high spectral resolution, HSI is extensively applied in various fields [3] including remote sensing [4], [5], environmental monitoring [6], food processing [7], red chili adulteration [8], [9], minced meat adulteration [10], [11], forensic [12], biomedical [13], [14], and many more. However, limited inherent challenges, labeled samples, and complex spectral-spatial variability, com- plicate the classification process [15]–[17]. including high dimensionality, These architectures leverage self-attention mechanisms that effectively capture long-range dependencies across both spec- tral and spatial dimensions, significantly enhancing feature representation [20]–[22]. Spatial-spectral transformers (SSTs) have emerged as a powerful alternative to convolutional neural networks (CNNs), focusing on modeling interactions between spectral bands and spatial contexts across patches [23]–[25]. SSTs exhibit excellent scalability when applied to high-resolution HSIs, efficiently handling large HSI datasets without the necessity for complex pooling operations. This capability has facilitated their successful use in HSI classifi- cation, with their flexible architecture contributing to wide- ranging applications. Additionally, SSTs minimize the de- pendence on manually engineered features by learning hi- erarchical representations directly from raw pixel data [26], streamlining model development and often leading to en- hanced performance. The attention maps produced by SSTs also improve interpretability by identifying the most influential image regions in the model’s predictions, thereby offering valuable insights into the decision-making process [27]. Several studies have explored transformer-based architec- tures for HSI classification. For example, Shi et al. [28] developed a dual-branch transformer network that effectively captures multi-scale spatial-spectral dependencies by sepa- rately processing spectral and spatial tokens. Lan et al. [29] in- troduced a hybrid-spatial transformer combining convolutional layers for better feature extraction. While these approaches show the effectiveness of transformers, challenges remain [30]. Training large SSTs is computationally demanding due to the quadratic complexity of self-attention, affecting scalability with longer sequences [31]. SSTs may also lack translation invariance compared to CNNs, impacting consistent spatial relationship capture [32]. The tokenization of input images into fixed-size patches can miss fine-grained details, and SSTs often need substantial training data, risking overfitting with smaller datasets [33]. Acquiring labeled HSI data is costly and time-consuming, limiting the feasibility of fully supervised methods [34], [35]. Thus, semi-supervised or active learning (AL) strategies that utilize both labeled and unlabeled data are essential for enhancing classification performance while reducing dependence on annotated samples [36], [37]. transformer-based architectures have gained prominence in the realm of HSI classification [18], [19]. Recently, M. Ahmad and S. Distefano are with Dipartimento di Matematica e Informatica-MIFT, University of Messina, 98121 Messina, Italy. (e-mail: mahmad00@gmail.com; sdistefano@unime.it M. Mazzara is with the Institute of Software Development and (e-mail: Engineering, m.mazzara@innopolis.ru) Innopolis University, Innopolis, 420500, Russia. AL aims to address the challenges of limited labeled data by selectively querying the most informative samples from a pool of data [38], thereby reducing annotation efforts while maximizing model performance [39], [40]. AL has been ef- fectively utilized in HSI classification to minimize annotation costs. For example, Ahmad et al. [41] implemented fuzziness- based sampling to iteratively update training sets, demon- strating improved performance with fewer labeled samples. 1 JOURNAL OF LATEX CLASS FILES Cho et al. [42] proposed an active deep learning framework for HSI classification, integrating CNN with AL to reduce labeling costs and enhance accuracy, with a Markov random field (MRF) refining classification by enforcing label smooth- ness across classes. Liu et al. [43] introduced MDL4OW, a multitask HSI classification that addresses unknown classes by performing simultaneous classification and reconstruction, identifying unknown classes through reconstruction errors. Liao et al. [44] presented CGE-AL, a class-wise graph- embedding-based AL framework that utilizes a class-wise graph convolutional network (CGCN) to optimize sample se- lection for labeling in HSI classification. By measuring uncer- tainty through graph parameters, CGE-AL iteratively queries and labels the most informative samples, thereby enhancing classification performance with minimal manual annotation. Zhao et al. [45] introduced MAT-ASSAL, an HSI classifica- tion framework that combines a multi-attention transformer (MAT) with adaptive superpixel segmentation-based AL to enhance classification performance. MAT captures long-range dependencies and fine-grained local features, while adaptive superpixel segmentation optimizes sample selection for train- ing, preserving essential spatial details. Wang et al. [46] proposed an AL approach that enhances deep classification performance by integrating a co-auxiliary learning strategy and multi-level diversity selection, where an auxiliary network aids in pseudo-labeling and reduces redundancy while ensuring the selection of both representative and uncertain samples. Liu et al. [47] developed an adversarial domain alignment framework with contrastive learning for HSI classification, which enhances cross-domain feature consistency for effective knowledge transfer. By combining spectral and semantic align- ment with multiscale feature selection, this method improves robustness and adaptability in few-shot scenarios using jointly optimized adversarial and contrastive losses. Despite the advancements, deep AL often struggles with the challenges posed by limited labeled samples, which constrain the diversity and quality of selected data, ultimately leading to suboptimal model generalization [48]. Additionally, HSIs present unique challenges due to their intricate spatial-spectral dependencies, which traditional AL methods may not fully capture [49]. These limitations underscore the necessity for an active transfer learning (ATL) framework, wherein an SST can leverage knowledge from labeled source domains to enhance sample selection and feature learning in low-labeled target domains. The integration of AL with transfer learning provides an innovative approach to these challenges, allowing models trained on a source domain to adapt efficiently to a target domain with minimal retraining. This methodology is particularly advantageous for dynamic remote sensing tasks, where the spectral characteristics of new images may differ significantly from those of the training set. By combining SST with ATL, the model can selectively query the most uncertain and diverse samples, iteratively updating the training set and thereby improving the model’s generalization capabilities in the target domain. In this study, we propose an ATL framework based on an SST (SST-ATL) specifically designed for HSI classification. This framework directly addresses the core challenges of', 'e79972bb-15a5-42e9-8e47-9853668ed68a\n\nII. PROPOSED METHODOLOGY Given an HSI X ∈ RM ×N ×k, where k denotes the number of spectral bands, M is the image height, and N is the width, the model initiates by reshaping X into a sequence of patches. This is done by dividing X into overlapping patches of size W × W . Pi = Conv3D(X, We), i = 1, . . . , Np where We are learnable weights of the embedding layer, and Np is the number of patches. Each patch Pi is of dimension Rp×p×k, and the total number of patches is given by Np = (cid:16) W . The embedding dimension is denoted by d. p (cid:17)2 A. Spectral-Spatial Transformer (SST) The SST leverages the self-attention mechanism within a transformer architecture to capture spectral and spatial depen- dencies in HSI. The model has three main components: patch 2) Positional Encoding: To incorporate information about the relative positions of the patches, we add a positional en- coding Epos to the patch embeddings. The positional encoding is computed as follows: 3 (1) JOURNAL OF LATEX CLASS FILES Epos(i, 2j) = sin Epos(i, 2j + 1) = cos (cid:18) i 10000 (cid:18) i 2j d (cid:19) , (cid:19) 10000 2j d where i is the position, j is the index of the dimension, and d is the embedding dimension. The resulting positional encoding is added to the patch embeddings: Z = P + Epos 3) Transformer Encoder Block: Each transformer en- coder block consists of a multi-head self-attention mechanism followed by a feedforward network (FFN). For each input patch embedding Zi, the self-attention mechanism computes attention scores and aggregates information from all patches. The multi-head attention is defined as: MultiHead(Q, K, V) = Concat(head1, . . . , headh)Wo where the queries, keys, and values are computed as: Qi = ZiWQ, Ki = ZiWK, Vi = ZiWV The attention scores are calculated using the scaled dot- product attention: Attention(Q, K, V) = Softmax (cid:32) QKT √ dk (cid:33) V where dk is the dimension of the keys and WQ, WK, WV are learnable weight matrices for the queries, keys, and values, respectively. The output of the multi-head attention layer is passed through a feedforward network consisting of two dense layers with a ReLU activation in between: F(Zi) = max(0, ZiW1 + b1)W2 + b2 The final output of the transformer encoder layer is given by: Z′ = LayerNorm(Z + Dropout(Attention(Z))) Z′′ = LayerNorm(Z′ + Dropout(F(Z′))) where LayerNorm represents layer normalization and Dropout is applied for regularization. 4) Cross-Attention Mechanism: The SST model integrates a cross-attention mechanism to enhance feature representation across patches. Let Qc ∈ Rd be a learnable query token and Kc, Vc be the keys and values derived from the patch embeddings. The cross-attention is computed as: (cid:18) QcKT c√ d 4) Cross-Attention Mechanism: The SST model integrates a cross-attention mechanism to enhance feature representation across patches. Let Qc ∈ Rd be a learnable query token and Kc, Vc be the keys and values derived from the patch embeddings. The cross-attention is computed as: (cid:18) QcKT c√ d Vc This allows the model to incorporate global context from all patches while maintaining spatial-spectral coherence. (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) 5) Classification Head: The output of the transformer encoder and cross-attention mechanisms are concatenated, followed by two fully connected layers with ReLU activations: O = ReLU(Z′′W3 + b3) Ofinal = Softmax(OW4 + b4) where Ofinal ∈ RC is the final class prediction, and C is the number of target classes. B. Active Learning We employ an active learning technique to iteratively query informative samples from a pool of data. The the most following query strategies are applied: 1) Hybrid Query Strategy: The hybrid query strategy combines uncertainty and diversity sampling. Let U and D denote the set of samples selected by uncertainty and diversity sampling, respectively. The final query set Q is formed as: Q = U(xi) ∪ D(xi) where Q ensures that both uncertain and diverse samples are included in the labeled set. The hybrid strategy combines uncertainty and diversity sampling to balance exploration and exploitation in the active learning setting. Uncertainty sam- pling selects samples for which the model is least confident. For a sample xi, the uncertainty score is calculated as: U(xi) = − max(p(y|xi)) where p(y|xi) is the predicted probability of the class for xi. Samples with the highest uncertainty scores are prioritized for labeling. Spatial-spectral neighborhood diversity sampling ensures that the selected pixels from the HSI are representative of diverse spatial-spectral characteristics within their local neighborhoods. This approach computes diversity based on the average pairwise spectral distance among neighboring pixels. Mathematically, this can be framed as: D(xi) = arg max S⊆Xpool,|S|=query size Diversity(S) where the diversity of a set S of pixels is defined as: Diversity(S) = 1 m(m − 1) m (cid:88) j=1 m (cid:88) k=1,k̸=j djk where djk is the pairwise Euclidean distance between the spectral features of pixels j and k within their respective neighborhoods, and m is the number of pixels in the local neighborhood defined as: m = (nneighborhood)2 The overall diversity for each pixel is computed as: diversity metric(h, w) = 1 m(m − 1) m (cid:88) j=1 m (cid:88) k=1,k̸=j djk 4 (12) (13) (14) (15) (16) (17) (18) (19) JOURNAL OF LATEX CLASS FILES where (h, w) represents the coordinates of the pixel in the HSI. The query indices of the most diverse pixels are selected based on: query indices = argsort(diversity values)[−query size :] (20) This ensures that the final selection of pixels captures the spatial-spectral diversity, enhancing the effectiveness of AL strategies. C. Model Training and Evaluation The SST model is trained using a categorical cross-entropy loss function: L = − N (cid:88) C (cid:88) yic log(ˆyic) c=1 where yic is the ground truth label and ˆyic is the predicted probability for class c for the i-th sample. The model is optimized using Adam with an initial learning rate of 1×10−4. We evaluate the model on']","The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.",multi_hop_abstract_query_synthesizer
6,"As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?","['48cdab7e-a2c8-4a01-9dc6-b72a7ba9adb1\n\n4 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639b8\n\n2. Related Works 2.1. Visual Object Tracking (VOT) Visual Object Tracking (VOT) [36] aims to track objects in challenging video sequences that include variations in ob- ject scale, occlusions, and complex backgrounds so as to elevate the robustness and accuracy of tracking algorithms. Siamese-based [10, 52] and transformer-based [12, 47] trackers are common approaches by learning embedding similarity. However, due to lacking self-correction of these trackers in the single forward pass evaluation scheme, they can easily drift toward distractors. To this end, recent works [18, 49] further introduce memory bank and attention to find a better mapping between current frame and history information. 2.2. Segment Anything Model (SAM) The Segment Anything Model (SAM) [26] has sparked con- siderable follow-up research since its introduction. SAM introduces a prompt-based segmentation approach, where users could input points, bounding boxes, or text to guide the model in segmenting any object within an image. The use of SAM has wide-ranging applications like in video un- derstanding [7, 38, 39] and editing [6]. Since then, various works have built upon SAM. For example, SAM 2 [35] ex- pands the model’s capabilities to video segmentation [11], incorporating memory mechanisms for tracking objects across multiple frames in dynamic video sequences. Ad- ditionally, efforts have been made to create more efficient variants of SAM for resource-constrained environments, aiming to reduce its computational demands [45, 54]. Re- search in medical imaging has also adopted SAM for spe- cialized tasks [30]. Recently, SAM2Long [14] uses tree- based memory to enhance object segmentation for long video. However, their higher FPS video sequences and deeper memory tree architectures require exponentially more computing power and memory storage due to the over- head of storing exact paths and time-constrained memory paths. On the other hand, our proposed SAMURAI model, which is built upon SAM 2, has been trained on large-scale segmentation datasets to overcome these challenges and en- sure good performance and generalization ability. 2.3. Motion Modeling Motion modeling is an important component in tracking tasks, which can be categorized into heuristic and learn- able approaches. Heuristic methods, such as the widely- used Kalman Filter (KF) [24], rely on fixed motion pri- ors and predefined hyper-parameters to predict object tra- jectories. While KF has proven effective in many track- ing benchmarks, it often fails in scenarios with intense or abrupt motion. Other methods [1] attempt to counteract intense or abrupt object motion by compensating for cam- era movement before applying traditional KF-based predic- 2 tion. However, both the standard and noise scale adaptive (NSA) Kalman Filters [15] come with a multitude of hyper- parameters, potentially restricting their effectiveness to spe- cific types of motion scenarios. In contrast, learnable mo- tion models have attracted increasing interest due to their data-driven nature. Tracktor [2] is the first to use trajec- tory boxes as Regions of Interest (RoI) in a Faster-RCNN to extract features and regress the object’s position across frames. MotionTrack [43] enhances tracking by learning past trajectory representations to predict future movements. MambaTrack [22] further explores different learning-based motion models architecture like transformer [40] and state- space model (SSM) [21]. Our approach is also a learning- based motion modeling with an enhanced heuristic scheme.', '6d638fba-501e-49fb-b7fe-f12c7cfc5635\n\n4. Method SAM 2 has demonstrated strong performance in basic Vi- sual Object Tracking (VOT) and Video Object Segmenta- tion (VOS) tasks. However, the original model can mistak- enly encode incorrect or low-confidence objects, leading to substantial error propagation in long-sequence VOT. To address the above issues, we propose a Kalman Fil- ter (KF)-based motion modeling on top of the multi-masks selection (in 4.1) and an enhanced memory selection based on a hybrid scoring system that combines affinity and mo- tion scores (in 4.2). These enhancements are designed to (2) (3) Figure 2. The overview of our SAMURAI visual object tracker. strengthen the model’s ability to track objects accurately in complex video scenarios. Importantly, this approach does not require fine-tuning, nor does it require additional train- ing, and it can be integrated directly into the existing SAM 2 model. By improving the selection of predicted masks with- out additional computational overhead, this method pro- vides a reliable, real-time solution for online VOT. M and the bounding box derived from the Kalman filter’s predicted state. We then select the mask that maximizes a weighted sum of the KF-IoU score and the original affinity score: M∗ = arg max (αkf · skf (Mi) + (1 − αkf ) · smask(Mi)). Mi 4.1. Motion Modeling Finally, the update is performed using: Motion modeling has long been an effective approach to Visual Object Tracking (VOT) and Multiple Object Track- ing (MOT) [1, 5, 51] in resolving association ambiguities. We employ the linear-based Kalman filter [24] as our base- line to demonstrate the incorporation of motion modeling in improving tracking accuracy. In our visual object tracking framework, we integrate the Kalman filter to enhance bounding box position and dimen- sion predictions, which in turn helps select the most confi- dent mask out of N candidates from M. We define the state vector x as: ˆxt|t = ˆxt|t−1 + Kt(zt − H ˆxt|t−1) where zt is the measurement, the bounding box derived from the mask we selected, used to update. F is the linear state transition matrix, Kn is the Kalman gain, and H is the observation matrix. Furthermore, to ensure the robustness of the motion modeling after the targeted object reappears or the poor mask qualities for a certain period of time, we also maintain a stable motion state where we take consider- ation of the motion module if and only if the tracked object is being successfully update in the past τkf frames. x = [x, y, w, h, ˙x, ˙y, ˙w, ˙h]T where x, y represents the center coordinate of the bound- ing box, w and h denote its width and height, respectively, and their corresponding velocities are represented by the dot notation. For each mask Mi, the corresponding bounding box di is derived by computing the minimum and maxi- mum x and y coordinates of the mask’s non-zero pixels. The Kalman filter operates in a predict-correct cycle, where the state prediction ˆxt+1|t is given by: (4)', '702254b9-86d6-469b-9ebd-fce417b63c5f\n\nI. INTRODUCTION H YPERSPECTRAL imaging (HSI) is a powerful remote in- sensing technique that captures detailed spectral formation across narrow wavelength bands, enabling precise identification and classification of diverse land-cover materials [1], [2]. Due to its high spectral resolution, HSI is extensively applied in various fields [3] including remote sensing [4], [5], environmental monitoring [6], food processing [7], red chili adulteration [8], [9], minced meat adulteration [10], [11], forensic [12], biomedical [13], [14], and many more. However, limited inherent challenges, labeled samples, and complex spectral-spatial variability, com- plicate the classification process [15]–[17]. including high dimensionality, These architectures leverage self-attention mechanisms that effectively capture long-range dependencies across both spec- tral and spatial dimensions, significantly enhancing feature representation [20]–[22]. Spatial-spectral transformers (SSTs) have emerged as a powerful alternative to convolutional neural networks (CNNs), focusing on modeling interactions between spectral bands and spatial contexts across patches [23]–[25]. SSTs exhibit excellent scalability when applied to high-resolution HSIs, efficiently handling large HSI datasets without the necessity for complex pooling operations. This capability has facilitated their successful use in HSI classifi- cation, with their flexible architecture contributing to wide- ranging applications. Additionally, SSTs minimize the de- pendence on manually engineered features by learning hi- erarchical representations directly from raw pixel data [26], streamlining model development and often leading to en- hanced performance. The attention maps produced by SSTs also improve interpretability by identifying the most influential image regions in the model’s predictions, thereby offering valuable insights into the decision-making process [27]. Several studies have explored transformer-based architec- tures for HSI classification. For example, Shi et al. [28] developed a dual-branch transformer network that effectively captures multi-scale spatial-spectral dependencies by sepa- rately processing spectral and spatial tokens. Lan et al. [29] in- troduced a hybrid-spatial transformer combining convolutional layers for better feature extraction. While these approaches show the effectiveness of transformers, challenges remain [30]. Training large SSTs is computationally demanding due to the quadratic complexity of self-attention, affecting scalability with longer sequences [31]. SSTs may also lack translation invariance compared to CNNs, impacting consistent spatial relationship capture [32]. The tokenization of input images into fixed-size patches can miss fine-grained details, and SSTs often need substantial training data, risking overfitting with smaller datasets [33]. Acquiring labeled HSI data is costly and time-consuming, limiting the feasibility of fully supervised methods [34], [35]. Thus, semi-supervised or active learning (AL) strategies that utilize both labeled and unlabeled data are essential for enhancing classification performance while reducing dependence on annotated samples [36], [37]. transformer-based architectures have gained prominence in the realm of HSI classification [18], [19]. Recently, M. Ahmad and S. Distefano are with Dipartimento di Matematica e Informatica-MIFT, University of Messina, 98121 Messina, Italy. (e-mail: mahmad00@gmail.com; sdistefano@unime.it M. Mazzara is with the Institute of Software Development and (e-mail: Engineering, m.mazzara@innopolis.ru) Innopolis University, Innopolis, 420500, Russia. AL aims to address the challenges of limited labeled data by selectively querying the most informative samples from a pool of data [38], thereby reducing annotation efforts while maximizing model performance [39], [40]. AL has been ef- fectively utilized in HSI classification to minimize annotation costs. For example, Ahmad et al. [41] implemented fuzziness- based sampling to iteratively update training sets, demon- strating improved performance with fewer labeled samples. 1 JOURNAL OF LATEX CLASS FILES Cho et al. [42] proposed an active deep learning framework for HSI classification, integrating CNN with AL to reduce labeling costs and enhance accuracy, with a Markov random field (MRF) refining classification by enforcing label smooth- ness across classes. Liu et al. [43] introduced MDL4OW, a multitask HSI classification that addresses unknown classes by performing simultaneous classification and reconstruction, identifying unknown classes through reconstruction errors. Liao et al. [44] presented CGE-AL, a class-wise graph- embedding-based AL framework that utilizes a class-wise graph convolutional network (CGCN) to optimize sample se- lection for labeling in HSI classification. By measuring uncer- tainty through graph parameters, CGE-AL iteratively queries and labels the most informative samples, thereby enhancing classification performance with minimal manual annotation. Zhao et al. [45] introduced MAT-ASSAL, an HSI classifica- tion framework that combines a multi-attention transformer (MAT) with adaptive superpixel segmentation-based AL to enhance classification performance. MAT captures long-range dependencies and fine-grained local features, while adaptive superpixel segmentation optimizes sample selection for train- ing, preserving essential spatial details. Wang et al. [46] proposed an AL approach that enhances deep classification performance by integrating a co-auxiliary learning strategy and multi-level diversity selection, where an auxiliary network aids in pseudo-labeling and reduces redundancy while ensuring the selection of both representative and uncertain samples. Liu et al. [47] developed an adversarial domain alignment framework with contrastive learning for HSI classification, which enhances cross-domain feature consistency for effective knowledge transfer. By combining spectral and semantic align- ment with multiscale feature selection, this method improves robustness and adaptability in few-shot scenarios using jointly optimized adversarial and contrastive losses. Despite the advancements, deep AL often struggles with the challenges posed by limited labeled samples, which constrain the diversity and quality of selected data, ultimately leading to suboptimal model generalization [48]. Additionally, HSIs present unique challenges due to their intricate spatial-spectral dependencies, which traditional AL methods may not fully capture [49]. These limitations underscore the necessity for an active transfer learning (ATL) framework, wherein an SST can leverage knowledge from labeled source domains to enhance sample selection and feature learning in low-labeled target domains. The integration of AL with transfer learning provides an innovative approach to these challenges, allowing models trained on a source domain to adapt efficiently to a target domain with minimal retraining. This methodology is particularly advantageous for dynamic remote sensing tasks, where the spectral characteristics of new images may differ significantly from those of the training set. By combining SST with ATL, the model can selectively query the most uncertain and diverse samples, iteratively updating the training set and thereby improving the model’s generalization capabilities in the target domain. In this study, we propose an ATL framework based on an SST (SST-ATL) specifically designed for HSI classification. This framework directly addresses the core challenges of', 'e79972bb-15a5-42e9-8e47-9853668ed68a\n\nII. PROPOSED METHODOLOGY Given an HSI X ∈ RM ×N ×k, where k denotes the number of spectral bands, M is the image height, and N is the width, the model initiates by reshaping X into a sequence of patches. This is done by dividing X into overlapping patches of size W × W . Pi = Conv3D(X, We), i = 1, . . . , Np where We are learnable weights of the embedding layer, and Np is the number of patches. Each patch Pi is of dimension Rp×p×k, and the total number of patches is given by Np = (cid:16) W . The embedding dimension is denoted by d. p (cid:17)2 A. Spectral-Spatial Transformer (SST) The SST leverages the self-attention mechanism within a transformer architecture to capture spectral and spatial depen- dencies in HSI. The model has three main components: patch 2) Positional Encoding: To incorporate information about the relative positions of the patches, we add a positional en- coding Epos to the patch embeddings. The positional encoding is computed as follows: 3 (1) JOURNAL OF LATEX CLASS FILES Epos(i, 2j) = sin Epos(i, 2j + 1) = cos (cid:18) i 10000 (cid:18) i 2j d (cid:19) , (cid:19) 10000 2j d where i is the position, j is the index of the dimension, and d is the embedding dimension. The resulting positional encoding is added to the patch embeddings: Z = P + Epos 3) Transformer Encoder Block: Each transformer en- coder block consists of a multi-head self-attention mechanism followed by a feedforward network (FFN). For each input patch embedding Zi, the self-attention mechanism computes attention scores and aggregates information from all patches. The multi-head attention is defined as: MultiHead(Q, K, V) = Concat(head1, . . . , headh)Wo where the queries, keys, and values are computed as: Qi = ZiWQ, Ki = ZiWK, Vi = ZiWV The attention scores are calculated using the scaled dot- product attention: Attention(Q, K, V) = Softmax (cid:32) QKT √ dk (cid:33) V where dk is the dimension of the keys and WQ, WK, WV are learnable weight matrices for the queries, keys, and values, respectively. The output of the multi-head attention layer is passed through a feedforward network consisting of two dense layers with a ReLU activation in between: F(Zi) = max(0, ZiW1 + b1)W2 + b2 The final output of the transformer encoder layer is given by: Z′ = LayerNorm(Z + Dropout(Attention(Z))) Z′′ = LayerNorm(Z′ + Dropout(F(Z′))) where LayerNorm represents layer normalization and Dropout is applied for regularization. 4) Cross-Attention Mechanism: The SST model integrates a cross-attention mechanism to enhance feature representation across patches. Let Qc ∈ Rd be a learnable query token and Kc, Vc be the keys and values derived from the patch embeddings. The cross-attention is computed as: (cid:18) QcKT c√ d 4) Cross-Attention Mechanism: The SST model integrates a cross-attention mechanism to enhance feature representation across patches. Let Qc ∈ Rd be a learnable query token and Kc, Vc be the keys and values derived from the patch embeddings. The cross-attention is computed as: (cid:18) QcKT c√ d Vc This allows the model to incorporate global context from all patches while maintaining spatial-spectral coherence. (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) 5) Classification Head: The output of the transformer encoder and cross-attention mechanisms are concatenated, followed by two fully connected layers with ReLU activations: O = ReLU(Z′′W3 + b3) Ofinal = Softmax(OW4 + b4) where Ofinal ∈ RC is the final class prediction, and C is the number of target classes. B. Active Learning We employ an active learning technique to iteratively query informative samples from a pool of data. The the most following query strategies are applied: 1) Hybrid Query Strategy: The hybrid query strategy combines uncertainty and diversity sampling. Let U and D denote the set of samples selected by uncertainty and diversity sampling, respectively. The final query set Q is formed as: Q = U(xi) ∪ D(xi) where Q ensures that both uncertain and diverse samples are included in the labeled set. The hybrid strategy combines uncertainty and diversity sampling to balance exploration and exploitation in the active learning setting. Uncertainty sam- pling selects samples for which the model is least confident. For a sample xi, the uncertainty score is calculated as: U(xi) = − max(p(y|xi)) where p(y|xi) is the predicted probability of the class for xi. Samples with the highest uncertainty scores are prioritized for labeling. Spatial-spectral neighborhood diversity sampling ensures that the selected pixels from the HSI are representative of diverse spatial-spectral characteristics within their local neighborhoods. This approach computes diversity based on the average pairwise spectral distance among neighboring pixels. Mathematically, this can be framed as: D(xi) = arg max S⊆Xpool,|S|=query size Diversity(S) where the diversity of a set S of pixels is defined as: Diversity(S) = 1 m(m − 1) m (cid:88) j=1 m (cid:88) k=1,k̸=j djk where djk is the pairwise Euclidean distance between the spectral features of pixels j and k within their respective neighborhoods, and m is the number of pixels in the local neighborhood defined as: m = (nneighborhood)2 The overall diversity for each pixel is computed as: diversity metric(h, w) = 1 m(m − 1) m (cid:88) j=1 m (cid:88) k=1,k̸=j djk 4 (12) (13) (14) (15) (16) (17) (18) (19) JOURNAL OF LATEX CLASS FILES where (h, w) represents the coordinates of the pixel in the HSI. The query indices of the most diverse pixels are selected based on: query indices = argsort(diversity values)[−query size :] (20) This ensures that the final selection of pixels captures the spatial-spectral diversity, enhancing the effectiveness of AL strategies. C. Model Training and Evaluation The SST model is trained using a categorical cross-entropy loss function: L = − N (cid:88) C (cid:88) yic log(ˆyic) c=1 where yic is the ground truth label and ˆyic is the predicted probability for class c for the i-th sample. The model is optimized using Adam with an initial learning rate of 1×10−4. We evaluate the model on']","The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.",multi_hop_abstract_query_synthesizer
7,how samurai help in visual object tracking?,"['48cdab7e-a2c8-4a01-9dc6-b72a7ba9adb1\n\n4 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639b8\n\n2. Related Works 2.1. Visual Object Tracking (VOT) Visual Object Tracking (VOT) [36] aims to track objects in challenging video sequences that include variations in ob- ject scale, occlusions, and complex backgrounds so as to elevate the robustness and accuracy of tracking algorithms. Siamese-based [10, 52] and transformer-based [12, 47] trackers are common approaches by learning embedding similarity. However, due to lacking self-correction of these trackers in the single forward pass evaluation scheme, they can easily drift toward distractors. To this end, recent works [18, 49] further introduce memory bank and attention to find a better mapping between current frame and history information. 2.2. Segment Anything Model (SAM) The Segment Anything Model (SAM) [26] has sparked con- siderable follow-up research since its introduction. SAM introduces a prompt-based segmentation approach, where users could input points, bounding boxes, or text to guide the model in segmenting any object within an image. The use of SAM has wide-ranging applications like in video un- derstanding [7, 38, 39] and editing [6]. Since then, various works have built upon SAM. For example, SAM 2 [35] ex- pands the model’s capabilities to video segmentation [11], incorporating memory mechanisms for tracking objects across multiple frames in dynamic video sequences. Ad- ditionally, efforts have been made to create more efficient variants of SAM for resource-constrained environments, aiming to reduce its computational demands [45, 54]. Re- search in medical imaging has also adopted SAM for spe- cialized tasks [30]. Recently, SAM2Long [14] uses tree- based memory to enhance object segmentation for long video. However, their higher FPS video sequences and deeper memory tree architectures require exponentially more computing power and memory storage due to the over- head of storing exact paths and time-constrained memory paths. On the other hand, our proposed SAMURAI model, which is built upon SAM 2, has been trained on large-scale segmentation datasets to overcome these challenges and en- sure good performance and generalization ability. 2.3. Motion Modeling Motion modeling is an important component in tracking tasks, which can be categorized into heuristic and learn- able approaches. Heuristic methods, such as the widely- used Kalman Filter (KF) [24], rely on fixed motion pri- ors and predefined hyper-parameters to predict object tra- jectories. While KF has proven effective in many track- ing benchmarks, it often fails in scenarios with intense or abrupt motion. Other methods [1] attempt to counteract intense or abrupt object motion by compensating for cam- era movement before applying traditional KF-based predic- 2 tion. However, both the standard and noise scale adaptive (NSA) Kalman Filters [15] come with a multitude of hyper- parameters, potentially restricting their effectiveness to spe- cific types of motion scenarios. In contrast, learnable mo- tion models have attracted increasing interest due to their data-driven nature. Tracktor [2] is the first to use trajec- tory boxes as Regions of Interest (RoI) in a Faster-RCNN to extract features and regress the object’s position across frames. MotionTrack [43] enhances tracking by learning past trajectory representations to predict future movements. MambaTrack [22] further explores different learning-based motion models architecture like transformer [40] and state- space model (SSM) [21]. Our approach is also a learning- based motion modeling with an enhanced heuristic scheme.', '6d638fba-501e-49fb-b7fe-f12c7cfc5635\n\n4. Method SAM 2 has demonstrated strong performance in basic Vi- sual Object Tracking (VOT) and Video Object Segmenta- tion (VOS) tasks. However, the original model can mistak- enly encode incorrect or low-confidence objects, leading to substantial error propagation in long-sequence VOT. To address the above issues, we propose a Kalman Fil- ter (KF)-based motion modeling on top of the multi-masks selection (in 4.1) and an enhanced memory selection based on a hybrid scoring system that combines affinity and mo- tion scores (in 4.2). These enhancements are designed to (2) (3) Figure 2. The overview of our SAMURAI visual object tracker. strengthen the model’s ability to track objects accurately in complex video scenarios. Importantly, this approach does not require fine-tuning, nor does it require additional train- ing, and it can be integrated directly into the existing SAM 2 model. By improving the selection of predicted masks with- out additional computational overhead, this method pro- vides a reliable, real-time solution for online VOT. M and the bounding box derived from the Kalman filter’s predicted state. We then select the mask that maximizes a weighted sum of the KF-IoU score and the original affinity score: M∗ = arg max (αkf · skf (Mi) + (1 − αkf ) · smask(Mi)). Mi 4.1. Motion Modeling Finally, the update is performed using: Motion modeling has long been an effective approach to Visual Object Tracking (VOT) and Multiple Object Track- ing (MOT) [1, 5, 51] in resolving association ambiguities. We employ the linear-based Kalman filter [24] as our base- line to demonstrate the incorporation of motion modeling in improving tracking accuracy. In our visual object tracking framework, we integrate the Kalman filter to enhance bounding box position and dimen- sion predictions, which in turn helps select the most confi- dent mask out of N candidates from M. We define the state vector x as: ˆxt|t = ˆxt|t−1 + Kt(zt − H ˆxt|t−1) where zt is the measurement, the bounding box derived from the mask we selected, used to update. F is the linear state transition matrix, Kn is the Kalman gain, and H is the observation matrix. Furthermore, to ensure the robustness of the motion modeling after the targeted object reappears or the poor mask qualities for a certain period of time, we also maintain a stable motion state where we take consider- ation of the motion module if and only if the tracked object is being successfully update in the past τkf frames. x = [x, y, w, h, ˙x, ˙y, ˙w, ˙h]T where x, y represents the center coordinate of the bound- ing box, w and h denote its width and height, respectively, and their corresponding velocities are represented by the dot notation. For each mask Mi, the corresponding bounding box di is derived by computing the minimum and maxi- mum x and y coordinates of the mask’s non-zero pixels. The Kalman filter operates in a predict-correct cycle, where the state prediction ˆxt+1|t is given by: (4)', '702254b9-86d6-469b-9ebd-fce417b63c5f\n\nI. INTRODUCTION H YPERSPECTRAL imaging (HSI) is a powerful remote in- sensing technique that captures detailed spectral formation across narrow wavelength bands, enabling precise identification and classification of diverse land-cover materials [1], [2]. Due to its high spectral resolution, HSI is extensively applied in various fields [3] including remote sensing [4], [5], environmental monitoring [6], food processing [7], red chili adulteration [8], [9], minced meat adulteration [10], [11], forensic [12], biomedical [13], [14], and many more. However, limited inherent challenges, labeled samples, and complex spectral-spatial variability, com- plicate the classification process [15]–[17]. including high dimensionality, These architectures leverage self-attention mechanisms that effectively capture long-range dependencies across both spec- tral and spatial dimensions, significantly enhancing feature representation [20]–[22]. Spatial-spectral transformers (SSTs) have emerged as a powerful alternative to convolutional neural networks (CNNs), focusing on modeling interactions between spectral bands and spatial contexts across patches [23]–[25]. SSTs exhibit excellent scalability when applied to high-resolution HSIs, efficiently handling large HSI datasets without the necessity for complex pooling operations. This capability has facilitated their successful use in HSI classifi- cation, with their flexible architecture contributing to wide- ranging applications. Additionally, SSTs minimize the de- pendence on manually engineered features by learning hi- erarchical representations directly from raw pixel data [26], streamlining model development and often leading to en- hanced performance. The attention maps produced by SSTs also improve interpretability by identifying the most influential image regions in the model’s predictions, thereby offering valuable insights into the decision-making process [27]. Several studies have explored transformer-based architec- tures for HSI classification. For example, Shi et al. [28] developed a dual-branch transformer network that effectively captures multi-scale spatial-spectral dependencies by sepa- rately processing spectral and spatial tokens. Lan et al. [29] in- troduced a hybrid-spatial transformer combining convolutional layers for better feature extraction. While these approaches show the effectiveness of transformers, challenges remain [30]. Training large SSTs is computationally demanding due to the quadratic complexity of self-attention, affecting scalability with longer sequences [31]. SSTs may also lack translation invariance compared to CNNs, impacting consistent spatial relationship capture [32]. The tokenization of input images into fixed-size patches can miss fine-grained details, and SSTs often need substantial training data, risking overfitting with smaller datasets [33]. Acquiring labeled HSI data is costly and time-consuming, limiting the feasibility of fully supervised methods [34], [35]. Thus, semi-supervised or active learning (AL) strategies that utilize both labeled and unlabeled data are essential for enhancing classification performance while reducing dependence on annotated samples [36], [37]. transformer-based architectures have gained prominence in the realm of HSI classification [18], [19]. Recently, M. Ahmad and S. Distefano are with Dipartimento di Matematica e Informatica-MIFT, University of Messina, 98121 Messina, Italy. (e-mail: mahmad00@gmail.com; sdistefano@unime.it M. Mazzara is with the Institute of Software Development and (e-mail: Engineering, m.mazzara@innopolis.ru) Innopolis University, Innopolis, 420500, Russia. AL aims to address the challenges of limited labeled data by selectively querying the most informative samples from a pool of data [38], thereby reducing annotation efforts while maximizing model performance [39], [40]. AL has been ef- fectively utilized in HSI classification to minimize annotation costs. For example, Ahmad et al. [41] implemented fuzziness- based sampling to iteratively update training sets, demon- strating improved performance with fewer labeled samples. 1 JOURNAL OF LATEX CLASS FILES Cho et al. [42] proposed an active deep learning framework for HSI classification, integrating CNN with AL to reduce labeling costs and enhance accuracy, with a Markov random field (MRF) refining classification by enforcing label smooth- ness across classes. Liu et al. [43] introduced MDL4OW, a multitask HSI classification that addresses unknown classes by performing simultaneous classification and reconstruction, identifying unknown classes through reconstruction errors. Liao et al. [44] presented CGE-AL, a class-wise graph- embedding-based AL framework that utilizes a class-wise graph convolutional network (CGCN) to optimize sample se- lection for labeling in HSI classification. By measuring uncer- tainty through graph parameters, CGE-AL iteratively queries and labels the most informative samples, thereby enhancing classification performance with minimal manual annotation. Zhao et al. [45] introduced MAT-ASSAL, an HSI classifica- tion framework that combines a multi-attention transformer (MAT) with adaptive superpixel segmentation-based AL to enhance classification performance. MAT captures long-range dependencies and fine-grained local features, while adaptive superpixel segmentation optimizes sample selection for train- ing, preserving essential spatial details. Wang et al. [46] proposed an AL approach that enhances deep classification performance by integrating a co-auxiliary learning strategy and multi-level diversity selection, where an auxiliary network aids in pseudo-labeling and reduces redundancy while ensuring the selection of both representative and uncertain samples. Liu et al. [47] developed an adversarial domain alignment framework with contrastive learning for HSI classification, which enhances cross-domain feature consistency for effective knowledge transfer. By combining spectral and semantic align- ment with multiscale feature selection, this method improves robustness and adaptability in few-shot scenarios using jointly optimized adversarial and contrastive losses. Despite the advancements, deep AL often struggles with the challenges posed by limited labeled samples, which constrain the diversity and quality of selected data, ultimately leading to suboptimal model generalization [48]. Additionally, HSIs present unique challenges due to their intricate spatial-spectral dependencies, which traditional AL methods may not fully capture [49]. These limitations underscore the necessity for an active transfer learning (ATL) framework, wherein an SST can leverage knowledge from labeled source domains to enhance sample selection and feature learning in low-labeled target domains. The integration of AL with transfer learning provides an innovative approach to these challenges, allowing models trained on a source domain to adapt efficiently to a target domain with minimal retraining. This methodology is particularly advantageous for dynamic remote sensing tasks, where the spectral characteristics of new images may differ significantly from those of the training set. By combining SST with ATL, the model can selectively query the most uncertain and diverse samples, iteratively updating the training set and thereby improving the model’s generalization capabilities in the target domain. In this study, we propose an ATL framework based on an SST (SST-ATL) specifically designed for HSI classification. This framework directly addresses the core challenges of', 'e79972bb-15a5-42e9-8e47-9853668ed68a\n\nII. PROPOSED METHODOLOGY Given an HSI X ∈ RM ×N ×k, where k denotes the number of spectral bands, M is the image height, and N is the width, the model initiates by reshaping X into a sequence of patches. This is done by dividing X into overlapping patches of size W × W . Pi = Conv3D(X, We), i = 1, . . . , Np where We are learnable weights of the embedding layer, and Np is the number of patches. Each patch Pi is of dimension Rp×p×k, and the total number of patches is given by Np = (cid:16) W . The embedding dimension is denoted by d. p (cid:17)2 A. Spectral-Spatial Transformer (SST) The SST leverages the self-attention mechanism within a transformer architecture to capture spectral and spatial depen- dencies in HSI. The model has three main components: patch 2) Positional Encoding: To incorporate information about the relative positions of the patches, we add a positional en- coding Epos to the patch embeddings. The positional encoding is computed as follows: 3 (1) JOURNAL OF LATEX CLASS FILES Epos(i, 2j) = sin Epos(i, 2j + 1) = cos (cid:18) i 10000 (cid:18) i 2j d (cid:19) , (cid:19) 10000 2j d where i is the position, j is the index of the dimension, and d is the embedding dimension. The resulting positional encoding is added to the patch embeddings: Z = P + Epos 3) Transformer Encoder Block: Each transformer en- coder block consists of a multi-head self-attention mechanism followed by a feedforward network (FFN). For each input patch embedding Zi, the self-attention mechanism computes attention scores and aggregates information from all patches. The multi-head attention is defined as: MultiHead(Q, K, V) = Concat(head1, . . . , headh)Wo where the queries, keys, and values are computed as: Qi = ZiWQ, Ki = ZiWK, Vi = ZiWV The attention scores are calculated using the scaled dot- product attention: Attention(Q, K, V) = Softmax (cid:32) QKT √ dk (cid:33) V where dk is the dimension of the keys and WQ, WK, WV are learnable weight matrices for the queries, keys, and values, respectively. The output of the multi-head attention layer is passed through a feedforward network consisting of two dense layers with a ReLU activation in between: F(Zi) = max(0, ZiW1 + b1)W2 + b2 The final output of the transformer encoder layer is given by: Z′ = LayerNorm(Z + Dropout(Attention(Z))) Z′′ = LayerNorm(Z′ + Dropout(F(Z′))) where LayerNorm represents layer normalization and Dropout is applied for regularization. 4) Cross-Attention Mechanism: The SST model integrates a cross-attention mechanism to enhance feature representation across patches. Let Qc ∈ Rd be a learnable query token and Kc, Vc be the keys and values derived from the patch embeddings. The cross-attention is computed as: (cid:18) QcKT c√ d 4) Cross-Attention Mechanism: The SST model integrates a cross-attention mechanism to enhance feature representation across patches. Let Qc ∈ Rd be a learnable query token and Kc, Vc be the keys and values derived from the patch embeddings. The cross-attention is computed as: (cid:18) QcKT c√ d Vc This allows the model to incorporate global context from all patches while maintaining spatial-spectral coherence. (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) 5) Classification Head: The output of the transformer encoder and cross-attention mechanisms are concatenated, followed by two fully connected layers with ReLU activations: O = ReLU(Z′′W3 + b3) Ofinal = Softmax(OW4 + b4) where Ofinal ∈ RC is the final class prediction, and C is the number of target classes. B. Active Learning We employ an active learning technique to iteratively query informative samples from a pool of data. The the most following query strategies are applied: 1) Hybrid Query Strategy: The hybrid query strategy combines uncertainty and diversity sampling. Let U and D denote the set of samples selected by uncertainty and diversity sampling, respectively. The final query set Q is formed as: Q = U(xi) ∪ D(xi) where Q ensures that both uncertain and diverse samples are included in the labeled set. The hybrid strategy combines uncertainty and diversity sampling to balance exploration and exploitation in the active learning setting. Uncertainty sam- pling selects samples for which the model is least confident. For a sample xi, the uncertainty score is calculated as: U(xi) = − max(p(y|xi)) where p(y|xi) is the predicted probability of the class for xi. Samples with the highest uncertainty scores are prioritized for labeling. Spatial-spectral neighborhood diversity sampling ensures that the selected pixels from the HSI are representative of diverse spatial-spectral characteristics within their local neighborhoods. This approach computes diversity based on the average pairwise spectral distance among neighboring pixels. Mathematically, this can be framed as: D(xi) = arg max S⊆Xpool,|S|=query size Diversity(S) where the diversity of a set S of pixels is defined as: Diversity(S) = 1 m(m − 1) m (cid:88) j=1 m (cid:88) k=1,k̸=j djk where djk is the pairwise Euclidean distance between the spectral features of pixels j and k within their respective neighborhoods, and m is the number of pixels in the local neighborhood defined as: m = (nneighborhood)2 The overall diversity for each pixel is computed as: diversity metric(h, w) = 1 m(m − 1) m (cid:88) j=1 m (cid:88) k=1,k̸=j djk 4 (12) (13) (14) (15) (16) (17) (18) (19) JOURNAL OF LATEX CLASS FILES where (h, w) represents the coordinates of the pixel in the HSI. The query indices of the most diverse pixels are selected based on: query indices = argsort(diversity values)[−query size :] (20) This ensures that the final selection of pixels captures the spatial-spectral diversity, enhancing the effectiveness of AL strategies. C. Model Training and Evaluation The SST model is trained using a categorical cross-entropy loss function: L = − N (cid:88) C (cid:88) yic log(ˆyic) c=1 where yic is the ground truth label and ˆyic is the predicted probability for class c for the i-th sample. The model is optimized using Adam with an initial learning rate of 1×10−4. We evaluate the model on']","SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.",multi_hop_abstract_query_synthesizer
8,How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?,"['c40ad8c2-1af8-4a6b-a3bf-b722a195985b\n\n2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) ≈ (cid:88) pη(z|x)pθ(y|x, z) = (cid:88) pη(z|x) N (cid:89) pθ(yi|x, z, y1:i−1) z∈top-k(p(·|x)) z∈top-k(p(·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token(y|x) ≈ N (cid:89) (cid:88) pη(z|x)pθ(yi|x, z, y1:i−1) i z∈top-k(p(·|x)) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j − log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To decode, we can plug p(cid:48) θ(yi|x, y1:i−1) = (cid:80) θ(yi|x, y1:i−1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”', '521b14e6-7228-4246-8929-6b41ae350481\n\n4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of ""open-book"" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding ""The Sun. BART completes the generation ""The Sun Also Rises"" is a novel by this author of ""The Sun Also Rises"" indicating the title ""The Sun Also Rises"" is stored in BART’s parameters. Similarly, BART will complete the partial decoding ""The Sun Also Rises"" is a novel by this author of ""A with ""The Sun Also Rises"" is a novel by this author of ""A Farewell to Arms"". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by ” this well a ” R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovel”AFarewelltoArms”(1929)...Document2:...artistsofthe1920s”LostGeneration”expatriatecommunity.Hisdebutnovel,”TheSunAlsoRises”,waspublishedin1926. is Fare BOS ises Doc3 ” Sun ” Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem- ingway"" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms"" and for document 2 when generating “The Sun Also Rises"". Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deﬁne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed']","The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.",multi_hop_specific_query_synthesizer
9,how RAG-Sequence model work with BERT and what it do for image classification?,"['c40ad8c2-1af8-4a6b-a3bf-b722a195985b\n\n2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) ≈ (cid:88) pη(z|x)pθ(y|x, z) = (cid:88) pη(z|x) N (cid:89) pθ(yi|x, z, y1:i−1) z∈top-k(p(·|x)) z∈top-k(p(·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token(y|x) ≈ N (cid:89) (cid:88) pη(z|x)pθ(yi|x, z, y1:i−1) i z∈top-k(p(·|x)) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j − log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To decode, we can plug p(cid:48) θ(yi|x, y1:i−1) = (cid:80) θ(yi|x, y1:i−1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”', '81a7d139-e415-4c40-8254-27bb6dc39105\n\nan interactive demo of a RAG model can be found at https://huggingface.co/rag/ 2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers 17 D Further Details on Open-Domain QA For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur in top 1000 documents for the query. CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres- sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace. TriviaQA Evaluation setups The open-domain QA community customarily uses public develop- ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being simpler to answer from Wikipedia. E Further Details on FEVER For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The ﬁrst is to classify the claim as either ""Supported"", ""Refuted"" or ""Not Enough Info"", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work. F Null Document Probabilities We experimented with adding ""Null document"" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally ""retrieve"" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document mechanisms may not be necessary for RAG. G Parameters Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable 18 Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation Task Train Development Test Natural Questions TriviaQA WebQuestions CuratedTrec Jeopardy Question Generation MS-MARCO FEVER-3-way FEVER-2-way 79169 78786 3418 635 97392 153726 145450 96966 8758 8838 362 134 13714 12468 10000 6666 3611 11314 2033 635 26849 101093* 10000 6666 parameters. The best performing ""closed-book"" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating point precision to manage memory and disk footprints. H Retrieval Collapse In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks. I Number of instances per dataset The number of training, development and test datapoints in each of our datasets is shown in Table 7. 19']","RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.",multi_hop_specific_query_synthesizer
10,Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?,"[""18dfba70-7a17-4642-8439-1c6ebe588e27\n\n2 Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 … Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Брзината на светлината во вакуум е 299 792 458 метри во секунда. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: Која е брзината на светлината? Multi-lingual LLMTransfer capabilities tolow-resource languages … Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: 光速是多少？ Bengali French Malayalam Marathi: प्रकाशाचा वेग किती आहे? French Spanish Portuguese: Qual é a velocidade da luz? Hindi Sinhalese: .,/ක 01ර3ය 01ර3ය 4ය5 ආලෙ$ක&ෙ 'ෙගය ත5පරයට 9ට: 299,792,458 1. Telugu Sinhalese: ආලෙ$ක&ෙ 'ෙගය *ම,ද? Catalan Marathi: उत्तर: प्रकाशाचा वेग 299,792,458 मीटर प्रति सेकंद आहे. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz é 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad är ljusets hastighet? Japanese: 光の速度はどれくらいですか? Swedish: Ljusets hastighet är 299 792 kilometer per sekund. Arabic Japanese: 光の速度は毎秒約 299792458 メートルです。 Nepali Danish Romanian Chinese: 光速是每秒299,792,458⽶。 … High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMs’ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMs’ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmålLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyèNajdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages."", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f2880\n\n5 Conclusion In this study, we develop BayLing 2, which enhances LLM’s multilingual capabilities through language alignment. Adhering to an efficiency-focused approach, BayLing 2 transfers knowledge and generative abilities from high-resource languages to low-resource languages within LLM via language alignment. Comprehensive evaluation results demonstrate that BayLing 2 achieves outstanding translation performance across over 100 languages, possesses superior multilingual knowledge and understanding capability, and maintains robust proficiency in high-resource languages of Chinese and English. Acknowledgements We extend our heartfelt gratitude to everyone who contributed to the development of BayLing 2. In particular, we would like to thank Ms. Xiaohong Wang for her insightful feedback and valuable suggestions regarding the use of InforSuperBahn MLOps, as well as for her exceptional support in organizing resources, providing computational infrastructure, and facilitating the presentation of BayLing 2. We also acknowledge the dedicated efforts of the development team at the Nanjing Institute of InforSuperBahn for their indispensable role in maintaining computational resources and designing the display interface for BayLing 2’s webpage and demonstration. References OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt. OpenAI. Gpt-4 technical report, 2023. Tyler A. Chang, Catherine Arnett, Zhuowen Tu, and Benjamin K. Bergen. When is multilinguality a curse? language modeling for 250 high- and low-resource languages, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, and Hanwen Gu. A survey on multilingual large language models: Corpora, alignment, and bias, 2024. Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and Lidong Bing. Seallms – large language models for southeast asia, 2023a. Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. Adapting pre-trained language models to african languages via multilingual adaptive fine-tuning, 2022. Qingcheng Zeng, Lucas Garay, Peilin Zhou, Dading Chong, Yining Hua, Jiageng Wu, Yikang Pan, Han Zhou, Rob Voigt, and Jie Yang. Greenplm: Cross-lingual transfer of monolingual pre-trained language models at almost no cost, 2023. 12 Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.acl-long.26. URL https://aclanthology.org/2022.acl-long.26. Jessica Ojo and Kelechi Ogueji. How good are commercial large language models on african languages?, 2023. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback, 2023. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023b. Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual pre-training of language models, 2023. Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. Continual pre-training of large language models: How to (re)warm your model?, 2023. Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, and Lidong Bing. Is translation all you need? a study on solving multilingual tasks with large language models, 2024. Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ninghao Liu, and Mengnan Du. Quantifying multilingual performance of large language models across languages, 2024. Julian Martin Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin Kardas, Sylvain Gugger, and Jeremy Howard. Multifit: Efficient multi-lingual language model fine-tuning, 2020. 13 Haoyu Wang, Shuo Wang, Yukun Yan, Xujia Wang, Zhiyu Yang, Yuzhuang Xu, Zhenghao Liu, Liner Yang, Ning Ding, Xu Han, Zhiyuan Liu, and Maosong Sun. Ultralink: An open-source knowledge-enhanced multilingual supervised fine-tuning dataset, 2024. Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. Multi- lingual instruction tuning with just a pinch of multilinguality, 2024. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep']","BayLing 2 enhances LLM’s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.",multi_hop_specific_query_synthesizer
11,"Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?","[""18dfba70-7a17-4642-8439-1c6ebe588e27\n\n2 Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 … Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Брзината на светлината во вакуум е 299 792 458 метри во секунда. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: Која е брзината на светлината? Multi-lingual LLMTransfer capabilities tolow-resource languages … Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: 光速是多少？ Bengali French Malayalam Marathi: प्रकाशाचा वेग किती आहे? French Spanish Portuguese: Qual é a velocidade da luz? Hindi Sinhalese: .,/ක 01ර3ය 01ර3ය 4ය5 ආලෙ$ක&ෙ 'ෙගය ත5පරයට 9ට: 299,792,458 1. Telugu Sinhalese: ආලෙ$ක&ෙ 'ෙගය *ම,ද? Catalan Marathi: उत्तर: प्रकाशाचा वेग 299,792,458 मीटर प्रति सेकंद आहे. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz é 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad är ljusets hastighet? Japanese: 光の速度はどれくらいですか? Swedish: Ljusets hastighet är 299 792 kilometer per sekund. Arabic Japanese: 光の速度は毎秒約 299792458 メートルです。 Nepali Danish Romanian Chinese: 光速是每秒299,792,458⽶。 … High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMs’ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMs’ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmålLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyèNajdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages."", '92c84056-7246-4e66-b74e-2162cb78c35a\n\n4 Evaluation In this section, we comprehensively evaluate the performance of BayLing-2-7B, BayLing-2-13B and BayLing-3-8B on multilingual tasks and general tasks respectively. 4.1 Multilingual Capability BayLing’s multilingual capabilities are primarily manifested in two aspects: multilingual translation and multilingual interaction. Multilingual translation aims to accomplish translation between different languages, which can be utilized to assess the language alignment within LLMs as well as the comprehension and generation capabilities across different languages. Multilingual interaction involves multitask language understanding using multiple languages, which can be employed to evaluate the multilingual knowledge and reasoning abilities of LLMs. 4.1.1 Multilingual Translation We employ multilingual translation to assess the multilingual alignment within LLMs, which entails producing outputs that retain the same meaning but in different languages. We conduct evaluation on the Flores-101 and WMT22 benchmarks. For metrics, BLEU (sacrebleu) [Post, 2018] and COMET [Rei et al., 2022] are used to assess the quality of LLMs’ translation. BLEU score measures the statistical similarity based on n-gram accuracy, COMET score measures the semantic similarity using cross-lingual pre-trained models, which is currently regarded as the most human-aligned evaluation metric for translation tasks. Flores-101 Flores-101 benchmark encompasses 101 languages from around the world, and the sentences is sourced from various domains, including news, travel guides and books. Due to the rarity of some low-resource languages, LLMs may suffer from off-target issues. To address this, we adopt a 1-shot setting (i.e., randomly selecting an example from the dev set) to help LLMs follow the 5 Table 1: Mulitlingual translation preformance on WMT22 benchmark. X indicates other 100 languages in Flores-101, and the results are averaged over these 100 languages. Models X⇒English English⇒X X⇒Chinese Chinese⇒X BLEU COMET BLEU COMET BLEU COMET BLEU COMET Llama-1-7B BayLing-1-7B 14.07 14.70 60.94 61.93 6.93 7.04 49.73 49.33 0.93 1.58 40.88 46.22 1.85 1.56 44.88 48.78 Llama-2-7B-Chat BayLing-2-7B 15.39 17.71 63.95 67.15 7.45 8.02 50.97 52.37 1.75 2.70 47.19 51.44 1.57 2.32 45.70 49.37 Llama-3-8B-Instruct BayLing-3-8B 25.20 26.77 76.60 77.03 16.59 17.91 67.17 70.88 11.79 11.31 71.91 69.43 8.95 10.64 63.57 67.86 (a) Flores-101 X⇒English (b) Flores-101 English⇒X Figure 6: English⇔101 languages translation performance on Flores-101 benchmark. target language through in-context learning. We compare BayLing models with their corresponding foundational models, and the results are shown in Table 1. The results in Table 1 indicate that BayLing achieves better performance in most translation directions between 100 languages and Chinese/English. Specifically, compared to the foundation LLMs Llama- 1-7B and Llama-2-7B-Chat, which have relatively weak multilingual capabilities, BayLing effectively scales their language understanding and generation capabilities to over 100 languages, leading to significantly improved translation performance. Furthermore, Figures 6(a), 6(b), 7(a) and 7(b) illustrate the specific BLEU score improvements achieved by BayLing across 100 languages. BayLing consistently delivers the highest translation quality for most languages, particularly in translation directions to low-resource languages. This demonstrates BayLing’s potential to enhance LLM in serving such low-source linguistic communities. 6 (a) Flores-101 X⇒Chinese (b) Flores-101 Chinese⇒X Figure 7: Chinese⇔101 languages translation performance on Flores-101 benchmark. WMT22 WMT22 benchmark6 encompass is used to evaluate high-resource multilingual trans- including translation directions of Chinese⇔English, German⇔English, lation performance, Czech⇔English, Japanese⇔English, Russian⇔English, and Ukrainian⇔English. We compared BayLing with the best closed-sourced and open-sourced models, including GPT-47 [OpenAI, 2023], GPT-3.5-turbo8 [OpenAI, 2022], Google Translate9, Llama[Touvron et al., 2023] and Vicuna [Chiang et al., 2023]. The translation results on WMT22 are shown in Table 2, where the results illustrate the superior multilingual translation capabilities of BayLing models. Among the open-sourced models, BayLing achieves the highest overall translation performance, coming remarkably close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. This exceptional performance can be attributed to BayLing’s improved language alignment, which enables it to produce more accurate and reliable translations across different languages. In particular,for the Zh⇔En translation, BayLing-3- 8B achieve a COMET score of 79.75 on Zh⇒En and 85.75 on En⇒Zh, which is very close to the performance of Google Translate. Improving Mulitlingual Generation Capabilities We have observed that foundational mod- els often exhibit off-target issues when generating low-resource languages. In contrast, BayLing demonstrates significantly enhanced multilingual generation capabilities, consistently improving translation performance from English to other languages. This indicates that BayLing can activate the multilingual generation abilities of LLMs solely through cross-lingual translation data, without the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering more than 100 languages while multilingual translation data is relatively abundant and easier to 6https://www.statmt.org/wmt22/translation-task.html 7We use GPT-4 API of version 0314 8We use GPT-3.5-turbo API 9https://translate.google.com/ 7 Table 2: Mulitlingual translation preformance on WMT22 benchmark. The bold and underlined results indicate the first and second best, respectively. Systems En⇒Zh En⇒De En⇒Cs En⇒Ja En⇒Ru En⇒Uk COMET BELU COMET BELU COMET BELU COMET BELU COMET BELU COMET BELU closed-sourced GPT-4 GPT-3.5-turbo Google Translate 87.49 86.81 87.34 43.98 44.99 49.89 87.44 86.93 87.08 35.38 34.12 38.27 90.77 90.05 91.28 34.53 32.71 48.10 89.87 83.26 88.64 24.71 22.22 26.50 88.87 87.52 88.91 30.45 29.59 35.04 88.46 87.43 88.63 26.71 25.87 32.05 open-sourced Llama-2-7B-Chat Llama-2-13B-Chat Vicuna-7B-v1.5 Vicuna-13B-v1.5 Llama-3-8B-Instruct BayLing-1-7B BayLing-1-13B BayLing-2-7B BayLing-2-13B BayLing-3-8B 67.90 75.23 81.40 84.01 80.55 84.43 84.62 85.94 86.65 85.75 17.50 24.31 29.54 34.69 30.10 38.19 37.92 39.71 42.87 41.49 72.22 77.25 75.25 81.99 82.18 82.18 82.69 82.76 83.79 84.53 16.74 20.35 16.65 24.22 25.83 25.66 25.62 25.65 26.61 29.59 65.17 75.42 71.84 77.97 83.24 76.85 78.22 80.54 82.93 87.55 11.69 16.18 13.63 17.47 23.41 15.64 16.43 17.81 18.52 24.57 69.66 78.46 74.80 85.45 65.43 71.23 71.39 84.94 83.60 83.34 9.52 13.56 11.28 17.54 10.57 4.51 6.05 16.43 15.79 16.82 67.60 77.19 77.66 83.31 82.92 74.72 71.01 82.03 85.23 86.79 12.47 17.11 17.95 21.60 23.53 14.85 12.77 19.72 22.95 26.41 66.94 75.41 74.96 81.32 80.69 76.01 66.83 75.85 80.89 85.97 10.65 14.75 13.26 17.86 18.88 11.66 8.32 12.25 14.60 21.81 Systems Zh⇒En De⇒En Cs⇒En Ja⇒En Ru⇒En Uk⇒En COMET BELU COMET BELU COMET BELU COMET BELU COMET BELU COMET BELU closed-sourced GPT-4 GPT-3.5-turbo Google Translate 82.79 82.64 80.81 27.20 26.13 28.63 85.62 85.47']","BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.",multi_hop_specific_query_synthesizer
