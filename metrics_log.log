2024-12-05 10:15:55,716 - metrics_logger - INFO - Loading Excel file from output.xlsx
2024-12-05 10:15:55,851 - metrics_logger - INFO - Calculating metrics for pairs: 'query' and 'retrieval'
2024-12-05 10:15:55,852 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and candidate: 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:15:55,853 - metrics_logger - INFO - Getting embeddings for the sentence: How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?
2024-12-05 10:15:59,437 - metrics_logger - INFO - Getting embeddings for the sentence: An interactive demo of RAG models can be found at https://huggingface.co/rag/
2
2024-12-05 10:16:14,048 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:16:14,049 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:16:14,049 - metrics_logger - INFO - Calculating Coverage Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:16:14,049 - metrics_logger - INFO - Calculating Relevance Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:16:19,866 - metrics_logger - INFO - Calculating all metrics for reference: 'What is the RAG-Sequnce model and how does it work?' and candidate: 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:16:19,867 - metrics_logger - INFO - Getting embeddings for the sentence: What is the RAG-Sequnce model and how does it work?
2024-12-05 10:16:21,891 - metrics_logger - INFO - Getting embeddings for the sentence: the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.
2024-12-05 10:16:36,601 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:16:36,602 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What is the RAG-Sequnce model and how does it work?' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:16:36,603 - metrics_logger - INFO - Calculating Coverage Score between 'What is the RAG-Sequnce model and how does it work?' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:16:36,603 - metrics_logger - INFO - Calculating Relevance Score between 'What is the RAG-Sequnce model and how does it work?' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:16:41,128 - metrics_logger - INFO - Calculating all metrics for reference: 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and candidate: '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:16:41,128 - metrics_logger - INFO - Getting embeddings for the sentence: As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?
2024-12-05 10:16:43,604 - metrics_logger - INFO - Getting embeddings for the sentence: [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,
2024-12-05 10:16:57,564 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:16:57,565 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:16:57,566 - metrics_logger - INFO - Calculating Coverage Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:16:57,566 - metrics_logger - INFO - Calculating Relevance Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:17:01,817 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht is the perfomance of T5 in open-domain QA tasks?' and candidate: 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:17:01,817 - metrics_logger - INFO - Getting embeddings for the sentence: Wht is the perfomance of T5 in open-domain QA tasks?
2024-12-05 10:17:04,197 - metrics_logger - INFO - Getting embeddings for the sentence: Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.
2024-12-05 10:17:16,860 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:17:16,861 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:17:16,861 - metrics_logger - INFO - Calculating Coverage Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:17:16,862 - metrics_logger - INFO - Calculating Relevance Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:17:21,651 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual tracking?' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:17:21,652 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual tracking?
2024-12-05 10:17:24,000 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 10:17:38,528 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:17:38,529 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:17:38,530 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:17:38,531 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:17:42,191 - metrics_logger - INFO - Calculating all metrics for reference: 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:17:42,191 - metrics_logger - INFO - Getting embeddings for the sentence: What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?
2024-12-05 10:17:44,625 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 10:17:59,041 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:17:59,042 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:17:59,043 - metrics_logger - INFO - Calculating Coverage Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:17:59,043 - metrics_logger - INFO - Calculating Relevance Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:18:02,920 - metrics_logger - INFO - Calculating all metrics for reference: 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:18:02,921 - metrics_logger - INFO - Getting embeddings for the sentence: As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?
2024-12-05 10:18:05,220 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 10:18:19,684 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:18:19,685 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:18:19,686 - metrics_logger - INFO - Calculating Coverage Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:18:19,686 - metrics_logger - INFO - Calculating Relevance Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:18:23,466 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual object tracking?' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:18:23,467 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual object tracking?
2024-12-05 10:18:25,513 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 10:18:37,985 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:18:37,986 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual object tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:18:37,987 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual object tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:18:37,987 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual object tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:18:41,603 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and candidate: 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:18:41,604 - metrics_logger - INFO - Getting embeddings for the sentence: How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?
2024-12-05 10:18:43,837 - metrics_logger - INFO - Getting embeddings for the sentence: retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
2024-12-05 10:18:56,687 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:18:56,688 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:18:56,689 - metrics_logger - INFO - Calculating Coverage Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:18:56,689 - metrics_logger - INFO - Calculating Relevance Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:19:00,537 - metrics_logger - INFO - Calculating all metrics for reference: 'how RAG-Sequence model work with BERT and what it do for image classification?' and candidate: 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:19:00,538 - metrics_logger - INFO - Getting embeddings for the sentence: how RAG-Sequence model work with BERT and what it do for image classification?
2024-12-05 10:19:02,525 - metrics_logger - INFO - Getting embeddings for the sentence: by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.
2024-12-05 10:19:15,975 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:19:15,976 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:19:15,976 - metrics_logger - INFO - Calculating Coverage Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:19:15,977 - metrics_logger - INFO - Calculating Relevance Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:19:19,971 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and candidate: 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:19:19,972 - metrics_logger - INFO - Getting embeddings for the sentence: Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?
2024-12-05 10:19:22,178 - metrics_logger - INFO - Getting embeddings for the sentence: The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short
2024-12-05 10:19:36,506 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:19:36,507 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:19:36,508 - metrics_logger - INFO - Calculating Coverage Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:19:36,508 - metrics_logger - INFO - Calculating Relevance Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:19:40,210 - metrics_logger - INFO - Calculating all metrics for reference: 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and candidate: 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:19:40,211 - metrics_logger - INFO - Getting embeddings for the sentence: Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?
2024-12-05 10:19:42,144 - metrics_logger - INFO - Getting embeddings for the sentence: In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7
2024-12-05 10:19:54,284 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:19:54,285 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:19:54,286 - metrics_logger - INFO - Calculating Coverage Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:19:54,287 - metrics_logger - INFO - Calculating Relevance Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:19:58,046 - metrics_logger - INFO - Calculating metrics for pairs: 'reference_retrieval' and 'retrieval'
2024-12-05 10:19:58,046 - metrics_logger - INFO - Calculating all metrics for reference: 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and candidate: 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:19:58,047 - metrics_logger - INFO - Getting embeddings for the sentence: Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.
2024-12-05 10:20:00,566 - metrics_logger - INFO - Getting embeddings for the sentence: An interactive demo of RAG models can be found at https://huggingface.co/rag/
2
2024-12-05 10:20:14,202 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:20:14,203 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:20:14,204 - metrics_logger - INFO - Calculating Coverage Score between 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:20:14,204 - metrics_logger - INFO - Calculating Relevance Score between 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:20:17,729 - metrics_logger - INFO - Calculating all metrics for reference: '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and candidate: 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:20:17,730 - metrics_logger - INFO - Getting embeddings for the sentence: 1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€
2024-12-05 10:20:22,957 - metrics_logger - INFO - Getting embeddings for the sentence: the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.
2024-12-05 10:20:37,583 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:20:37,584 - metrics_logger - INFO - Calculating Jaccard Similarity between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:20:37,587 - metrics_logger - INFO - Calculating Coverage Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:20:37,588 - metrics_logger - INFO - Calculating Relevance Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 10:20:41,789 - metrics_logger - INFO - Calculating all metrics for reference: 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and candidate: '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:20:41,790 - metrics_logger - INFO - Getting embeddings for the sentence: Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
2024-12-05 10:20:47,170 - metrics_logger - INFO - Getting embeddings for the sentence: [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,
2024-12-05 10:21:02,034 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:21:02,035 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:21:02,036 - metrics_logger - INFO - Calculating Coverage Score between 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:21:02,037 - metrics_logger - INFO - Calculating Relevance Score between 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 10:21:12,415 - metrics_logger - INFO - Calculating all metrics for reference: 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and candidate: 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:21:12,416 - metrics_logger - INFO - Getting embeddings for the sentence: Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed
2024-12-05 10:21:18,718 - metrics_logger - INFO - Getting embeddings for the sentence: Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.
2024-12-05 10:21:32,017 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:21:32,017 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:21:32,019 - metrics_logger - INFO - Calculating Coverage Score between 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:21:32,020 - metrics_logger - INFO - Calculating Relevance Score between 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 10:21:36,657 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:21:36,658 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 10:21:39,444 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 10:21:52,257 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:21:52,258 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:21:52,259 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:21:52,260 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:21:56,481 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:21:56,482 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 10:21:59,260 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 10:22:11,828 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:22:11,829 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:11,830 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:11,830 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:19,676 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:19,677 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 10:22:22,560 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 10:22:36,021 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:22:36,022 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:36,023 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:36,024 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 10:22:43,962 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:22:43,963 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 10:22:46,711 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 10:22:59,283 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:22:59,284 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:22:59,285 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:22:59,285 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 10:23:03,036 - metrics_logger - INFO - Calculating all metrics for reference: '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and candidate: 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:23:03,037 - metrics_logger - INFO - Getting embeddings for the sentence: 1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504
2024-12-05 10:23:09,022 - metrics_logger - INFO - Getting embeddings for the sentence: retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
2024-12-05 10:23:21,474 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:23:21,475 - metrics_logger - INFO - Calculating Jaccard Similarity between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:23:21,476 - metrics_logger - INFO - Calculating Coverage Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:23:21,477 - metrics_logger - INFO - Calculating Relevance Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 10:23:25,173 - metrics_logger - INFO - Calculating all metrics for reference: '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and candidate: 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:23:25,173 - metrics_logger - INFO - Getting embeddings for the sentence: 1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391
2024-12-05 10:23:30,650 - metrics_logger - INFO - Getting embeddings for the sentence: by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.
2024-12-05 10:23:44,258 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:23:44,259 - metrics_logger - INFO - Calculating Jaccard Similarity between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:23:44,260 - metrics_logger - INFO - Calculating Coverage Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:23:44,261 - metrics_logger - INFO - Calculating Relevance Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 10:23:56,823 - metrics_logger - INFO - Calculating all metrics for reference: 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and candidate: 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:23:56,824 - metrics_logger - INFO - Getting embeddings for the sentence: Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28
2024-12-05 10:24:05,445 - metrics_logger - INFO - Getting embeddings for the sentence: The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short
2024-12-05 10:24:17,629 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:24:17,629 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:24:17,630 - metrics_logger - INFO - Calculating Coverage Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:24:17,630 - metrics_logger - INFO - Calculating Relevance Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 10:24:22,030 - metrics_logger - INFO - Calculating all metrics for reference: 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and candidate: 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:24:22,031 - metrics_logger - INFO - Getting embeddings for the sentence: Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3
2024-12-05 10:24:31,657 - metrics_logger - INFO - Getting embeddings for the sentence: In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7
2024-12-05 10:24:45,031 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:24:45,032 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:24:45,033 - metrics_logger - INFO - Calculating Coverage Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:24:45,034 - metrics_logger - INFO - Calculating Relevance Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 10:24:49,046 - metrics_logger - INFO - Calculating metrics for pairs: 'query' and 'candidate'
2024-12-05 10:24:49,047 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and candidate: 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:24:49,047 - metrics_logger - INFO - Getting embeddings for the sentence: How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?
2024-12-05 10:24:51,124 - metrics_logger - INFO - Getting embeddings for the sentence: The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.
2024-12-05 10:34:20,778 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:34:20,779 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:34:20,780 - metrics_logger - INFO - Calculating Coverage Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:34:20,780 - metrics_logger - INFO - Calculating Relevance Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:34:25,623 - metrics_logger - INFO - Calculating all metrics for reference: 'What is the RAG-Sequnce model and how does it work?' and candidate: 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:34:25,624 - metrics_logger - INFO - Getting embeddings for the sentence: What is the RAG-Sequnce model and how does it work?
2024-12-05 10:34:27,613 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.
2024-12-05 10:34:40,693 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:34:40,694 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What is the RAG-Sequnce model and how does it work?' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:34:40,695 - metrics_logger - INFO - Calculating Coverage Score between 'What is the RAG-Sequnce model and how does it work?' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:34:40,695 - metrics_logger - INFO - Calculating Relevance Score between 'What is the RAG-Sequnce model and how does it work?' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:34:44,635 - metrics_logger - INFO - Calculating all metrics for reference: 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and candidate: 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:34:44,636 - metrics_logger - INFO - Getting embeddings for the sentence: As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?
2024-12-05 10:34:46,615 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.
2024-12-05 10:34:59,368 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:34:59,369 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:34:59,370 - metrics_logger - INFO - Calculating Coverage Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:34:59,370 - metrics_logger - INFO - Calculating Relevance Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:35:03,350 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht is the perfomance of T5 in open-domain QA tasks?' and candidate: 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:35:03,351 - metrics_logger - INFO - Getting embeddings for the sentence: Wht is the perfomance of T5 in open-domain QA tasks?
2024-12-05 10:35:05,507 - metrics_logger - INFO - Getting embeddings for the sentence: The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.
2024-12-05 10:35:19,253 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:35:19,255 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:35:19,256 - metrics_logger - INFO - Calculating Coverage Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:35:19,256 - metrics_logger - INFO - Calculating Relevance Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:35:22,855 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual tracking?' and candidate: 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:35:22,856 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual tracking?
2024-12-05 10:35:24,958 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.
2024-12-05 10:35:38,936 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:35:38,937 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual tracking?' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:35:38,938 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual tracking?' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:35:38,939 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual tracking?' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:35:42,488 - metrics_logger - INFO - Calculating all metrics for reference: 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and candidate: 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:35:42,489 - metrics_logger - INFO - Getting embeddings for the sentence: What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?
2024-12-05 10:35:44,668 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.
2024-12-05 10:35:58,376 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:35:58,377 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:35:58,377 - metrics_logger - INFO - Calculating Coverage Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:35:58,378 - metrics_logger - INFO - Calculating Relevance Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:36:02,134 - metrics_logger - INFO - Calculating all metrics for reference: 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and candidate: 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:36:02,135 - metrics_logger - INFO - Getting embeddings for the sentence: As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?
2024-12-05 10:36:04,089 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.
2024-12-05 10:36:18,621 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:36:18,622 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:36:18,623 - metrics_logger - INFO - Calculating Coverage Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:36:18,624 - metrics_logger - INFO - Calculating Relevance Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:36:22,203 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual object tracking?' and candidate: 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:36:22,204 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual object tracking?
2024-12-05 10:36:24,158 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how SAMURAI helps in visual object tracking.
2024-12-05 10:36:37,023 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:36:37,024 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual object tracking?' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:36:37,025 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual object tracking?' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:36:37,026 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual object tracking?' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:36:40,974 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and candidate: 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:36:40,975 - metrics_logger - INFO - Getting embeddings for the sentence: How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?
2024-12-05 10:36:42,912 - metrics_logger - INFO - Getting embeddings for the sentence: The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.
2024-12-05 10:36:54,975 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:36:54,976 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:36:54,977 - metrics_logger - INFO - Calculating Coverage Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:36:54,978 - metrics_logger - INFO - Calculating Relevance Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:36:58,617 - metrics_logger - INFO - Calculating all metrics for reference: 'how RAG-Sequence model work with BERT and what it do for image classification?' and candidate: 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:36:58,618 - metrics_logger - INFO - Getting embeddings for the sentence: how RAG-Sequence model work with BERT and what it do for image classification?
2024-12-05 10:37:00,761 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.
2024-12-05 10:37:13,581 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:37:13,582 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:37:13,583 - metrics_logger - INFO - Calculating Coverage Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:37:13,583 - metrics_logger - INFO - Calculating Relevance Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:37:17,548 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and candidate: 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:37:17,550 - metrics_logger - INFO - Getting embeddings for the sentence: Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?
2024-12-05 10:37:19,564 - metrics_logger - INFO - Getting embeddings for the sentence: The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.
2024-12-05 10:37:32,514 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:37:32,515 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:37:32,515 - metrics_logger - INFO - Calculating Coverage Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:37:32,516 - metrics_logger - INFO - Calculating Relevance Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:37:36,186 - metrics_logger - INFO - Calculating all metrics for reference: 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and candidate: 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:37:36,187 - metrics_logger - INFO - Getting embeddings for the sentence: Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?
2024-12-05 10:37:38,954 - metrics_logger - INFO - Getting embeddings for the sentence: The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.
2024-12-05 10:37:52,415 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:37:52,416 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:37:52,417 - metrics_logger - INFO - Calculating Coverage Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:37:52,418 - metrics_logger - INFO - Calculating Relevance Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:37:56,516 - metrics_logger - INFO - Calculating metrics for pairs: 'reference' and 'candidate'
2024-12-05 10:37:56,517 - metrics_logger - INFO - Calculating all metrics for reference: 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and candidate: 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:37:56,517 - metrics_logger - INFO - Getting embeddings for the sentence: The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.
2024-12-05 10:37:58,585 - metrics_logger - INFO - Getting embeddings for the sentence: The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.
2024-12-05 10:38:09,573 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:38:09,574 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:38:09,575 - metrics_logger - INFO - Calculating Coverage Score between 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:38:09,575 - metrics_logger - INFO - Calculating Relevance Score between 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 10:38:13,307 - metrics_logger - INFO - Calculating all metrics for reference: 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and candidate: 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:38:13,307 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.
2024-12-05 10:38:15,657 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.
2024-12-05 10:38:28,412 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:38:28,413 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:38:28,414 - metrics_logger - INFO - Calculating Coverage Score between 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:38:28,414 - metrics_logger - INFO - Calculating Relevance Score between 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 10:38:32,429 - metrics_logger - INFO - Calculating all metrics for reference: 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and candidate: 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:38:32,430 - metrics_logger - INFO - Getting embeddings for the sentence: In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.
2024-12-05 10:38:34,921 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.
2024-12-05 10:38:48,131 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:38:48,131 - metrics_logger - INFO - Calculating Jaccard Similarity between 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:38:48,132 - metrics_logger - INFO - Calculating Coverage Score between 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:38:48,132 - metrics_logger - INFO - Calculating Relevance Score between 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 10:38:52,125 - metrics_logger - INFO - Calculating all metrics for reference: 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and candidate: 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:38:52,126 - metrics_logger - INFO - Getting embeddings for the sentence: On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.
2024-12-05 10:38:54,624 - metrics_logger - INFO - Getting embeddings for the sentence: The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.
2024-12-05 10:39:06,852 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:39:06,853 - metrics_logger - INFO - Calculating Jaccard Similarity between 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:39:06,853 - metrics_logger - INFO - Calculating Coverage Score between 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:39:06,854 - metrics_logger - INFO - Calculating Relevance Score between 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 10:39:10,784 - metrics_logger - INFO - Calculating all metrics for reference: 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and candidate: 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:39:10,785 - metrics_logger - INFO - Getting embeddings for the sentence: SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.
2024-12-05 10:39:13,303 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.
2024-12-05 10:39:26,775 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:39:26,776 - metrics_logger - INFO - Calculating Jaccard Similarity between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:39:26,777 - metrics_logger - INFO - Calculating Coverage Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:39:26,777 - metrics_logger - INFO - Calculating Relevance Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 10:39:30,836 - metrics_logger - INFO - Calculating all metrics for reference: 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and candidate: 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:39:30,837 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.
2024-12-05 10:39:33,218 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.
2024-12-05 10:39:45,941 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:39:45,942 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:39:45,943 - metrics_logger - INFO - Calculating Coverage Score between 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:39:45,945 - metrics_logger - INFO - Calculating Relevance Score between 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 10:39:50,734 - metrics_logger - INFO - Calculating all metrics for reference: 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and candidate: 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:39:50,734 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.
2024-12-05 10:39:53,180 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.
2024-12-05 10:40:08,056 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:40:08,057 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:40:08,058 - metrics_logger - INFO - Calculating Coverage Score between 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:40:08,060 - metrics_logger - INFO - Calculating Relevance Score between 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 10:40:12,302 - metrics_logger - INFO - Calculating all metrics for reference: 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and candidate: 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:40:12,303 - metrics_logger - INFO - Getting embeddings for the sentence: SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.
2024-12-05 10:40:14,905 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how SAMURAI helps in visual object tracking.
2024-12-05 10:40:31,301 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:40:31,301 - metrics_logger - INFO - Calculating Jaccard Similarity between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:40:31,302 - metrics_logger - INFO - Calculating Coverage Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:40:31,303 - metrics_logger - INFO - Calculating Relevance Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 10:40:36,423 - metrics_logger - INFO - Calculating all metrics for reference: 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and candidate: 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:40:36,424 - metrics_logger - INFO - Getting embeddings for the sentence: The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.
2024-12-05 10:40:39,089 - metrics_logger - INFO - Getting embeddings for the sentence: The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.
2024-12-05 10:40:56,742 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:40:56,743 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:40:56,743 - metrics_logger - INFO - Calculating Coverage Score between 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:40:56,744 - metrics_logger - INFO - Calculating Relevance Score between 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 10:41:00,778 - metrics_logger - INFO - Calculating all metrics for reference: 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and candidate: 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:41:00,779 - metrics_logger - INFO - Getting embeddings for the sentence: RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.
2024-12-05 10:41:03,273 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.
2024-12-05 10:41:16,487 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:41:16,488 - metrics_logger - INFO - Calculating Jaccard Similarity between 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:41:16,488 - metrics_logger - INFO - Calculating Coverage Score between 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:41:16,488 - metrics_logger - INFO - Calculating Relevance Score between 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 10:41:21,722 - metrics_logger - INFO - Calculating all metrics for reference: 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and candidate: 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:41:21,723 - metrics_logger - INFO - Getting embeddings for the sentence: BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.
2024-12-05 10:41:24,847 - metrics_logger - INFO - Getting embeddings for the sentence: The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.
2024-12-05 10:41:38,973 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:41:38,974 - metrics_logger - INFO - Calculating Jaccard Similarity between 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:41:38,974 - metrics_logger - INFO - Calculating Coverage Score between 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:41:38,976 - metrics_logger - INFO - Calculating Relevance Score between 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 10:41:43,003 - metrics_logger - INFO - Calculating all metrics for reference: 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and candidate: 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:41:43,004 - metrics_logger - INFO - Getting embeddings for the sentence: BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.
2024-12-05 10:41:47,069 - metrics_logger - INFO - Getting embeddings for the sentence: The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.
2024-12-05 10:42:01,532 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 10:42:01,532 - metrics_logger - INFO - Calculating Jaccard Similarity between 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:42:01,533 - metrics_logger - INFO - Calculating Coverage Score between 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:42:01,534 - metrics_logger - INFO - Calculating Relevance Score between 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 10:42:06,150 - metrics_logger - INFO - Saving results to output_metrics.xlsx
2024-12-05 10:59:52,064 - metrics_logger - INFO - Loading Excel file from output.xlsx
2024-12-05 10:59:52,269 - metrics_logger - INFO - Calculating metrics for pairs: 'query' and 'retrieval'
2024-12-05 10:59:52,270 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and candidate: 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 10:59:52,270 - metrics_logger - INFO - Getting embeddings for the sentence: How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?
2024-12-05 10:59:55,705 - metrics_logger - INFO - Getting embeddings for the sentence: An interactive demo of RAG models can be found at https://huggingface.co/rag/
2
2024-12-05 11:00:10,137 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:00:10,138 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:00:10,138 - metrics_logger - INFO - Calculating Coverage Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:00:10,139 - metrics_logger - INFO - Calculating Relevance Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:00:18,268 - metrics_logger - INFO - Calculating all metrics for reference: 'What is the RAG-Sequnce model and how does it work?' and candidate: 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:00:18,269 - metrics_logger - INFO - Getting embeddings for the sentence: What is the RAG-Sequnce model and how does it work?
2024-12-05 11:00:20,448 - metrics_logger - INFO - Getting embeddings for the sentence: the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.
2024-12-05 11:00:34,098 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:00:34,099 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What is the RAG-Sequnce model and how does it work?' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:00:34,132 - metrics_logger - INFO - Calculating Coverage Score between 'What is the RAG-Sequnce model and how does it work?' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:00:34,132 - metrics_logger - INFO - Calculating Relevance Score between 'What is the RAG-Sequnce model and how does it work?' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:00:39,532 - metrics_logger - INFO - Calculating all metrics for reference: 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and candidate: '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:00:39,595 - metrics_logger - INFO - Getting embeddings for the sentence: As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?
2024-12-05 11:00:42,142 - metrics_logger - INFO - Getting embeddings for the sentence: [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,
2024-12-05 11:00:56,057 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:00:56,058 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:00:56,058 - metrics_logger - INFO - Calculating Coverage Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:00:56,058 - metrics_logger - INFO - Calculating Relevance Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:01:01,033 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht is the perfomance of T5 in open-domain QA tasks?' and candidate: 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:01:01,034 - metrics_logger - INFO - Getting embeddings for the sentence: Wht is the perfomance of T5 in open-domain QA tasks?
2024-12-05 11:01:03,414 - metrics_logger - INFO - Getting embeddings for the sentence: Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.
2024-12-05 11:01:17,627 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:01:17,627 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:01:17,628 - metrics_logger - INFO - Calculating Coverage Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:01:17,628 - metrics_logger - INFO - Calculating Relevance Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:01:22,423 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual tracking?' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:01:22,424 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual tracking?
2024-12-05 11:01:24,698 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 11:01:39,459 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:01:39,459 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:01:39,460 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:01:39,460 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:01:43,671 - metrics_logger - INFO - Calculating all metrics for reference: 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:01:43,674 - metrics_logger - INFO - Getting embeddings for the sentence: What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?
2024-12-05 11:01:46,345 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 11:02:00,657 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:02:00,658 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:00,658 - metrics_logger - INFO - Calculating Coverage Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:00,658 - metrics_logger - INFO - Calculating Relevance Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:05,350 - metrics_logger - INFO - Calculating all metrics for reference: 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:05,350 - metrics_logger - INFO - Getting embeddings for the sentence: As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?
2024-12-05 11:02:07,903 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 11:02:22,768 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:02:22,769 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:22,770 - metrics_logger - INFO - Calculating Coverage Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:22,770 - metrics_logger - INFO - Calculating Relevance Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:02:27,368 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual object tracking?' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:02:27,369 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual object tracking?
2024-12-05 11:02:29,730 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 11:02:40,899 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:02:40,899 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual object tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:02:40,900 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual object tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:02:40,900 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual object tracking?' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:02:45,505 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and candidate: 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:02:45,506 - metrics_logger - INFO - Getting embeddings for the sentence: How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?
2024-12-05 11:02:47,688 - metrics_logger - INFO - Getting embeddings for the sentence: retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
2024-12-05 11:02:58,977 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:02:58,977 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:02:58,978 - metrics_logger - INFO - Calculating Coverage Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:02:58,979 - metrics_logger - INFO - Calculating Relevance Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:03:02,934 - metrics_logger - INFO - Calculating all metrics for reference: 'how RAG-Sequence model work with BERT and what it do for image classification?' and candidate: 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:03:02,935 - metrics_logger - INFO - Getting embeddings for the sentence: how RAG-Sequence model work with BERT and what it do for image classification?
2024-12-05 11:03:05,004 - metrics_logger - INFO - Getting embeddings for the sentence: by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.
2024-12-05 11:03:17,790 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:03:17,791 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:03:17,791 - metrics_logger - INFO - Calculating Coverage Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:03:17,791 - metrics_logger - INFO - Calculating Relevance Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:03:21,725 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and candidate: 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:03:21,726 - metrics_logger - INFO - Getting embeddings for the sentence: Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?
2024-12-05 11:03:23,847 - metrics_logger - INFO - Getting embeddings for the sentence: The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short
2024-12-05 11:03:36,737 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:03:36,738 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:03:36,738 - metrics_logger - INFO - Calculating Coverage Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:03:36,738 - metrics_logger - INFO - Calculating Relevance Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:03:40,553 - metrics_logger - INFO - Calculating all metrics for reference: 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and candidate: 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:03:40,553 - metrics_logger - INFO - Getting embeddings for the sentence: Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?
2024-12-05 11:03:42,865 - metrics_logger - INFO - Getting embeddings for the sentence: In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7
2024-12-05 11:03:54,969 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:03:54,970 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:03:54,970 - metrics_logger - INFO - Calculating Coverage Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:03:54,971 - metrics_logger - INFO - Calculating Relevance Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:03:59,245 - metrics_logger - INFO - Calculating metrics for pairs: 'reference_retrieval' and 'retrieval'
2024-12-05 11:03:59,246 - metrics_logger - INFO - Calculating all metrics for reference: 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and candidate: 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:03:59,246 - metrics_logger - INFO - Getting embeddings for the sentence: Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.
2024-12-05 11:04:02,105 - metrics_logger - INFO - Getting embeddings for the sentence: An interactive demo of RAG models can be found at https://huggingface.co/rag/
2
2024-12-05 11:04:13,899 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:04:13,900 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:04:13,900 - metrics_logger - INFO - Calculating Coverage Score between 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:04:13,902 - metrics_logger - INFO - Calculating Relevance Score between 'Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pÎ·(z|x) with parameters Î· that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pÎ¸(yi|x, z, y1:iâˆ’1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by Î¸ that generates a current token based on a context of the previous i âˆ’ 1 tokens y1:iâˆ’1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pÎ· and pÎ¸ components, as well as the training and decoding procedure.' and 'An interactive demo of RAG models can be found at https://huggingface.co/rag/
2'
2024-12-05 11:04:17,993 - metrics_logger - INFO - Calculating all metrics for reference: '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and candidate: 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:04:17,993 - metrics_logger - INFO - Getting embeddings for the sentence: 1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€
2024-12-05 11:04:23,970 - metrics_logger - INFO - Getting embeddings for the sentence: the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.
2024-12-05 11:04:36,814 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:04:36,815 - metrics_logger - INFO - Calculating Jaccard Similarity between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:04:36,817 - metrics_logger - INFO - Calculating Coverage Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:04:36,817 - metrics_logger - INFO - Calculating Relevance Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€' and 'the non-parametric memory can be replaced to update the modelsâ€™ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documents z and use them
as additional context when generating the target sequence y . As shown in Figure 1, our models
leverage two components: (i) a retriever p Î·(z |x ) with parameters Î· that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator p Î¸(y i |x,z,y 1:i âˆ’1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/.'
2024-12-05 11:04:41,128 - metrics_logger - INFO - Calculating all metrics for reference: 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and candidate: '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:04:41,129 - metrics_logger - INFO - Getting embeddings for the sentence: Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
2024-12-05 11:04:46,890 - metrics_logger - INFO - Getting embeddings for the sentence: [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,
2024-12-05 11:04:59,905 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:04:59,906 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:04:59,909 - metrics_logger - INFO - Calculating Coverage Score between 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:04:59,910 - metrics_logger - INFO - Calculating Relevance Score between 'Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k âˆˆ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as â€œWhat is the weather in Volcano, CA?â€ so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAGâ€™s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, â€œThe World Cupâ€ is the answer to the question â€œIn 1986 Mexico scored as the ï¬rst country to host this international sports competition twice.â€ As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciï¬city. We deï¬ne factuality as whether a statement can be corroborated by trusted external sources, and speciï¬city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four optionsâ€”quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriï¬cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriï¬able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG modelsâ€™ ability to handle classiï¬cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals arenâ€™t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiï¬cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.' and '[31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
kdocuments for each query. We consider kâˆˆ{5,10}for training and set kfor test time using dev
data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to â€œClosed-Book
QAâ€ approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAGâ€™s natural language generation (NLG) in a knowledge-intensive setting,'
2024-12-05 11:05:11,061 - metrics_logger - INFO - Calculating all metrics for reference: 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and candidate: 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:05:11,062 - metrics_logger - INFO - Getting embeddings for the sentence: Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed
2024-12-05 11:05:18,091 - metrics_logger - INFO - Getting embeddings for the sentence: Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.
2024-12-05 11:05:30,513 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:05:30,514 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:05:30,514 - metrics_logger - INFO - Calculating Coverage Score between 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:05:30,515 - metrics_logger - INFO - Calculating Relevance Score between 'Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ï¬‚exibility of the â€œclosed-bookâ€ (parametric only) approaches and the performance of "open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized â€œsalient span maskingâ€ pre-training [20]. It is worth noting that RAGâ€™s retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross- encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - /50.1 37.4 /60.5 44.7 - Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - 40.7 46.8 41.1 50.6 SotA BART 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 92.2* 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciï¬c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ï¬nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see Â§4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ï¬nd RAG generations to be more speciï¬c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating â€œSunâ€, the posterior is high for document 2 which mentions â€œThe Sun Also Risesâ€. Similarly, document 1 dominates the posterior when â€œA Farewell to Armsâ€ is generated. Intriguingly, after the ï¬rst token of each book is generated, the document posterior ï¬‚attens. This observation suggests that the generator can complete the titles without depending on speciï¬c documents. In other words, the modelâ€™s parametric knowledge is sufï¬cient to complete the titles. We ï¬nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun Also Rises" indicating the title "The Sun Also Rises" is stored in BARTâ€™s parameters. Similarly, BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows how parametric and non-parametric memories work togetherâ€”the non-parametric component helps to guide the generation, drawing out speciï¬c knowledge stored in the parametric memory. 4.4 Fact Veriï¬cation Table 2 shows our results on FEVER. For 3-way classiï¬cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciï¬c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 author novel Doc2 to Doc4 Doc1 by â€ this well a â€ R The Also A Doc5 Arms of Document1:hisworksareconsideredclassicsofAmericanliterature...Hiswartimeexperiencesformedthebasisforhisnovelâ€AFarewelltoArmsâ€(1929)...Document2:...artistsofthe1920sâ€LostGenerationâ€expatriatecommunity.Hisdebutnovel,â€TheSunAlsoRisesâ€,waspublishedin1926. is Fare BOS ises Doc3 â€ Sun â€ Figure 2: RAG-Token document posterior p(zi|x, yi, yâˆ’i) for each generated token for input â€œHem- ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating â€œA Farewell to Arms" and for document 2 when generating â€œThe Sun Also Rises". Table 3: Examples from generation tasks. RAG models generate more speciï¬c and factually accurate responses. â€˜?â€™ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deï¬ne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed' and 'Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details. Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiï¬cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [ 57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok.'
2024-12-05 11:05:34,564 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:05:34,565 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 11:05:37,572 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 11:05:49,198 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:05:49,198 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:05:49,199 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:05:49,199 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:05:53,179 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:05:53,180 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 11:05:56,208 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 11:06:09,415 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:06:09,416 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:09,416 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:09,419 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:18,015 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:18,016 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 11:06:21,142 - metrics_logger - INFO - Getting embeddings for the sentence: Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-
2024-12-05 11:06:34,218 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:06:34,219 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:34,219 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:34,220 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Introduction
Segment Anything Model (SAM) [26] has demonstrated
impressive performance in segmentation tasks. Recently,
SAM 2 [35] incorporates a streaming memory architecture,
which enables it to process video frames sequentially while
maintaining context over long sequences. While SAM 2 has
shown remarkable capabilities in Video Object Segmenta-
tion (VOS [46]) tasks, generating precise pixel-level masks
for objects throughout a video sequence, it still faces chal-
lenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent
object identity and location despite occlusions, appearance
changes, and the presence of similar objects. However,
SAM 2 often neglects motion cues when predicting masks
for subsequent frames, leading to inaccuracies in scenarios
with rapid object movement or complex interactions. This
limitation is particularly evident in crowded scenes, where
SAM 2 tends to prioritize appearance similarity over spatial
and temporal consistency, resulting in tracking errors. As
illustrated in Figure 1, there are two common failure pat-
terns: confusion in crowded scenes and ineffective memory
utilization during occlusions. To address these limitations, we propose incorporating
motion information into SAM 2â€™s prediction process. By
leveraging the history of object trajectories, we can enhance
the modelâ€™s ability to differentiate between visually simi-
lar objects and maintain tracking accuracy in the presence
of occlusions. Additionally, optimizing SAM 2â€™s memory
management is crucial. The current approach [14, 35] of in-
discriminately storing recent frames in the memory bank in-
troduces irrelevant features during occlusions, compromis-
ing tracking performance. Addressing these challenges is
essential to adapt SAM 2â€™s rich mask information for ro-
bust video object tracking. To this end, we propose SAMURAI, a SAM-based
Unified and Robust zero-shot visual tracker with motion-
Aware Instance-level memory. Our proposed method in-
corporates two key advancements: (1) a motion modeling
system that refines the mask selection, enabling more ac-
curate object position prediction in complex scenarios, and
(2) an optimized memory selection mechanism that lever-'
2024-12-05 11:06:43,012 - metrics_logger - INFO - Calculating all metrics for reference: '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and candidate: 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:06:43,013 - metrics_logger - INFO - Getting embeddings for the sentence: 2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639
2024-12-05 11:06:46,167 - metrics_logger - INFO - Getting embeddings for the sentence: Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.
2024-12-05 11:06:56,970 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:06:56,971 - metrics_logger - INFO - Calculating Jaccard Similarity between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:06:56,971 - metrics_logger - INFO - Calculating Coverage Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:06:56,972 - metrics_logger - INFO - Calculating Relevance Score between '2 0 2 v o N 8 1 ] V C . s c [ 1 v 2 2 9 1 1 . 1 1 4 2 : v i X r a SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai Zhongyu Jiang Jenq-Neng Hwang University of Washington {cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu Abstract lenges in Visual Object Tracking (VOT [36]) scenarios. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when man- aging crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of mem- ories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incor- porating temporal motion cues with the proposed motion- aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demon- strates strong zero-shot performance across diverse bench- mark datasets, showcasing its ability to generalize with- out fine-tuning. In evaluations, SAMURAI achieves signif- icant improvements in success rate and precision over ex- isting trackers, with a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Moreover, it achieves compet- itive results compared to fully supervised methods on La- SOT, underscoring its robustness in complex tracking sce- narios and its potential for real-world applications in dy- namic environments. Code and results are available at https://github.com/yangchris11/samurai.', '42d0ed7a-4c61-4eb9-b08f-1ac895c639' and 'Figure 2. The overview of our SAMURAI visual object tracker. strengthen the modelâ€™s ability to track objects accurately in
complex video scenarios.'
2024-12-05 11:07:00,815 - metrics_logger - INFO - Calculating all metrics for reference: '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and candidate: 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:07:00,816 - metrics_logger - INFO - Getting embeddings for the sentence: 1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504
2024-12-05 11:07:07,124 - metrics_logger - INFO - Getting embeddings for the sentence: retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
2024-12-05 11:07:19,969 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:07:19,970 - metrics_logger - INFO - Calculating Jaccard Similarity between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:07:19,973 - metrics_logger - INFO - Calculating Coverage Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:07:19,973 - metrics_logger - INFO - Calculating Relevance Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '521b14e6-7228-4246-8929-6b41ae3504' and 'retriever is initialized using DPRâ€™s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based â€œcross-
encoderâ€ to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.'
2024-12-05 11:07:23,816 - metrics_logger - INFO - Calculating all metrics for reference: '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and candidate: 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:07:23,817 - metrics_logger - INFO - Getting embeddings for the sentence: 1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391
2024-12-05 11:07:29,877 - metrics_logger - INFO - Getting embeddings for the sentence: by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.
2024-12-05 11:07:44,016 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:07:44,017 - metrics_logger - INFO - Calculating Jaccard Similarity between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:07:44,017 - metrics_logger - INFO - Calculating Coverage Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:07:44,020 - metrics_logger - INFO - Calculating Relevance Score between '1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) zâˆˆtop-k(p(Â·|x)) zâˆˆtop-k(p(Â·|x)) i RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne: pRAG-Token(y|x) â‰ˆ N (cid:89) (cid:88) pÎ·(z|x)pÎ¸(yi|x, z, y1:iâˆ’1) i zâˆˆtop-k(p(Â·|x)) Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pÎ·(z|x) âˆ exp (cid:0)d(z)(cid:62)q(x)(cid:1) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pÎ·(Â·|x)), the list of k documents z with highest prior probability pÎ·(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pÎ¸(yi|x, z, y1:iâˆ’1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters Î¸ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ï¬ne-tuning training corpus of input/output pairs (xj, yj), we 3 minimize the negative marginal log-likelihood of each target, (cid:80) j âˆ’ log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ï¬nd this step necessary for strong performance, and keep the document encoder (and index) ï¬xed, only ï¬ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p(cid:48) zâˆˆtop-k(p(Â·|x)) pÎ·(zi|x)pÎ¸(yi|x, zi, y1:iâˆ’1) To decode, we can plug p(cid:48) Î¸(yi|x, y1:iâˆ’1) = (cid:80) Î¸(yi|x, y1:iâˆ’1) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pÎ¸(yi|x, z, y1:iâˆ’1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pÎ·(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as â€œThorough Decoding.â€ For longer output sequences, |Y | can become large, requiring many forward passes. For more efï¬cient decoding, we can make a further approximation that pÎ¸(y|x, zi) â‰ˆ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as â€œFast Decoding.â€', '81a7d139-e415-4c40-8254-27bb6dc391' and 'by Î¸that generates a current token based on a context of the previous iâˆ’1 tokens y1:iâˆ’1, the original
input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pÎ· and pÎ¸ components, as well as the training and decoding procedure. 2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) â‰ˆ
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(y|x,z) =
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)
Nâˆ
i
pÎ¸(yi|x,z,y 1:iâˆ’1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deï¬ne:
pRAG-Token(y|x) â‰ˆ
Nâˆ
i
âˆ‘
zâˆˆtop-k(p(Â·|x))
pÎ·(z|x)pÎ¸(yi|x,z,y 1:iâˆ’1)
Finally, we note that RAG can be used for sequence classiï¬cation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR
The retrieval component pÎ·(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pÎ·(z|x) âˆexp
(
d(z)âŠ¤q(x)
)
d(z) =BERTd(z), q(x) =BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pÎ·(Â·|x)), the list of kdocuments zwith highest prior probability pÎ·(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index.'
2024-12-05 11:07:57,899 - metrics_logger - INFO - Calculating all metrics for reference: 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and candidate: 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:07:57,900 - metrics_logger - INFO - Getting embeddings for the sentence: Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28
2024-12-05 11:08:07,403 - metrics_logger - INFO - Getting embeddings for the sentence: The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short
2024-12-05 11:08:18,800 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:08:18,800 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:08:18,803 - metrics_logger - INFO - Calculating Coverage Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:08:18,804 - metrics_logger - INFO - Calculating Relevance Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", 'cfa2caeb-bd2c-408e-9793-2d6ddc6f28' and 'The superior multilingual translation capabilities on Flores-101 and WMT22 underscores BayLingâ€™s
potential as a leading tool in the field of multilingual translation, offering significant advancements in
multilingual capabilities of LLM. 4.1.2 Multilingual Multi-task Evaluation
We assessed the multilingual performance of BayLing using several benchmarks. All evaluations
were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source,
unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result
was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct,
Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are
discribed as follows. Belebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension
benchmark, which evaluates mono- and multi-lingual models across different resource levels with
rigorously checked questions. Each question has four multiple-choice answers and is linked to a short'
2024-12-05 11:08:23,189 - metrics_logger - INFO - Calculating all metrics for reference: 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and candidate: 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:08:23,189 - metrics_logger - INFO - Getting embeddings for the sentence: Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3
2024-12-05 11:08:32,516 - metrics_logger - INFO - Getting embeddings for the sentence: In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7
2024-12-05 11:08:45,127 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:08:45,128 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:08:45,128 - metrics_logger - INFO - Calculating Coverage Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:08:45,129 - metrics_logger - INFO - Calculating Relevance Score between 'Related Work Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these 2 â€¦ Chinese BayLing 2 English: What is the speed of light? Hindi Marathi Macedonian: Ð‘Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð° Ð²Ð¾ Ð²Ð°ÐºÑƒÑƒÐ¼ Ðµ 299 792 458 Ð¼ÐµÑ‚Ñ€Ð¸ Ð²Ð¾ ÑÐµÐºÑƒÐ½Ð´Ð°. Dutch Serbian Slovak Cross-lingual Instructions Ukrainian Hungarian Vietnamese GermanLow-resource LanguageInstructions Bengali German Croatian Dutch Macedonian: ÐšÐ¾Ñ˜Ð° Ðµ Ð±Ñ€Ð·Ð¸Ð½Ð°Ñ‚Ð° Ð½Ð° ÑÐ²ÐµÑ‚Ð»Ð¸Ð½Ð°Ñ‚Ð°? Multi-lingual LLMTransfer capabilities tolow-resource languages â€¦ Italian English Arabic English: The speed of light is approximately 299,792 kilometers per second. Chinese: å…‰é€Ÿæ˜¯å¤šå°‘ï¼Ÿ Bengali French Malayalam Marathi: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— à¤•à¤¿à¤¤à¥€ à¤†à¤¹à¥‡? French Spanish Portuguese: Qual Ã© a velocidade da luz? Hindi Sinhalese: .,/à¶š 01à¶»3à¶º 01à¶»3à¶º 4à¶º5 à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º à¶­5à¶´à¶»à¶ºà¶§ 9à¶§: 299,792,458 1. Telugu Sinhalese: à¶†à¶½à·™$à¶š&à·™ 'à·™à¶œà¶º *à¶¸,à¶¯? Catalan Marathi: à¤‰à¤¤à¥à¤¤à¤°: à¤ªà¥à¤°à¤•à¤¾à¤¶à¤¾à¤šà¤¾ à¤µà¥‡à¤— 299,792,458 à¤®à¥€à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿ à¤¸à¥‡à¤•à¤‚à¤¦ à¤†à¤¹à¥‡. Slovak Kannada Spanish Tamil (cid:335)(cid:335) English Nepali Portuguese: A velocidade da luz Ã© 299.792.458 metros por segundo. Italian Telugu Tamil Swedish: Vad Ã¤r ljusets hastighet? Japanese: å…‰ã®é€Ÿåº¦ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹? Swedish: Ljusets hastighet Ã¤r 299 792 kilometer per sekund. Arabic Japanese: å…‰ã®é€Ÿåº¦ã¯æ¯Žç§’ç´„ 299792458 ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚ Nepali Danish Romanian Chinese: å…‰é€Ÿæ˜¯æ¯ç§’299,792,458â½¶ã€‚ â€¦ High-resource LanguageInstructions Chinese Danish Figure 1: Overview of BayLing 2. BayLing 2 is a multilingual LLM with efficient language alignment. BayLing 2 designates Chinese and English, two high-resource languages, as pivot languages and applies cross-lingual tasks to align 100+ languages to these pivot languages, which facilitates the capabilities transfer from high-resource languages to low-resource languages. During inference, BayLing 2 is capable of high-quality interaction across multiple languages. corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages. Current approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations. For supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMsâ€™ ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMsâ€™ instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring 3 106 107 ChineseEnglishKoreanPortugueseSpanishRussianVietnameseItalianTamilTeluguGermanFrenchGujaratiNorwegianJapanesePunjabiArabicIndonesianSwahiliNepaliSomaliBengaliPersianMalayalamTagalogHindiMarathiCatalanTurkishUrduPolishDutchAfrikaansRomanianKannadaChinese (Traditional)ThaiOdiaLaoGreekSwedishFinnishKhmerHebrewUkrainianEstonianCzechBurmeseAmharicDanishBulgarianCroatianWelshLithuanianAssameseHungarianSlovakMacedonianSantaliArmenianShanGeorgianCentral KurdishSindhiSlovenianEastern PanjabiPanjabiCentral Atlas TamazightTamasheq (Tifinagh script)SinhalaDzongkhaTigrinyaTajikYorubaNorth AzerbaijaniStandard TibetanKazakhBelarusianIgboMaoriIrishMalteseTraditional ChineseZuluMeitei (Bengali script)NyanjaShonaXhosaIcelandicCebuanoKambaHausaGandaUmbunduLuxembourgishLingalaWolofLuoSerbianJavaneseBosnianOccitanModern GreekUyghurKabuverdianuNorwegian BokmÃ¥lLatvianMaithiliGalicianSanskritAsturianEastern YiddishAwadhiBhojpuriMagahiKashmiri (Devanagari script)ChhattisgarhiKashmiri (Arabic script)PushtoSouthern PashtoMinangkabau (Arabic script)Banjar (Arabic script)Acehnese (Arabic script)Western PersianDariCentral Kanuri (Arabic script)South AzerbaijaniKabiyÃ¨Najdi ArabicModern Standard ArabicTaizzi-Adeni ArabicEgyptian ArabicMoroccan ArabicMesopotamian ArabicTunisian ArabicSouth Levantine ArabicNorth Levantine ArabicBashkirFonNuerHalh MongolianMongolianKikuyuTatarTumbukaKirghizKyrgyzScottish GaelicModern Standard Arabic (Romanized)EweTosk AlbanianJingphoWest Central OromoOromoTurkmenAkanBembaCentral Kanuri (Latin script)TsongaTswanaMossiPlateau MalagasySamoanCrimean TatarStandard LatvianSouthern SothoNorthern KurdishYue ChineseKinyarwandaNorthern UzbekCentral AymaraUzbekRundiAlbanianKimbunduNorthern SothoSwatiPediTwiWarayIlocanoFijianLatgalianGuaraniBambaraKabyleAyacucho QuechuaTok PisinTamasheq (Latin script)SilesianSangoChokweKikongoAcehnese (Latin script)FaroeseLuba-KasaiSouthwestern DinkaBugineseDyulaMalayLombardStandard MalayMizoLigurianSardinianSicilianBasqueMinangkabau (Latin script)BalineseBanjar (Latin script)SundaneseNigerian FulfuldeFulahFriulianEsperantoPapiamentoLimburgishHaitian CreoleVenetianNorwegian NynorskPangasinanLanguages 108Tokens Figure 2: Language distribution of instruction dataset. Chinese43.2%English26.2%Cross-lingual30.6% 105 104 106Number of Instructionsnews & social concerndiaries & daily lifefitness & healthfashion & stylesportsother hobbiesscience & technologyfood & diningarts & culturebusiness & entrepreneursmusiclearning & educationalrelationshipsfilm tv & videoyouth & student lifegamingcelebrity & pop culturetravel & adventurefamilyotherInstruction Topic 103 Figure 3: Distribution of in- struction categories, including Chinese, English and cross- lingual instructions. Figure 4: Distribution of the tokens number involved in each instruction. knowledge, instruction-following, and generation abilities from high-resource languages to low- resource languages.", '92c84056-7246-4e66-b74e-2162cb78c3' and 'In contrast, BayLing
demonstrates significantly enhanced multilingual generation capabilities, consistently improving
translation performance from English to other languages. This indicates that BayLing can activate
the multilingual generation abilities of LLMs solely through cross-lingual translation data, without
the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing
the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering
more than 100 languages while multilingual translation data is relatively abundant and easier to
6https://www.statmt.org/wmt22/translation-task.html
7We use GPT-4 API of version 0314
8We use GPT-3.5-turbo API
9https://translate.google.com/
7'
2024-12-05 11:08:49,129 - metrics_logger - INFO - Calculating metrics for pairs: 'query' and 'candidate'
2024-12-05 11:08:49,130 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and candidate: 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:08:49,131 - metrics_logger - INFO - Getting embeddings for the sentence: How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?
2024-12-05 11:08:51,496 - metrics_logger - INFO - Getting embeddings for the sentence: The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.
2024-12-05 11:09:03,329 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:09:03,329 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:09:03,330 - metrics_logger - INFO - Calculating Coverage Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:09:03,330 - metrics_logger - INFO - Calculating Relevance Score between 'How does the HuggingFace Transformers Library facilitate the implementation of RAG models in machine learning research?' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:09:07,289 - metrics_logger - INFO - Calculating all metrics for reference: 'What is the RAG-Sequnce model and how does it work?' and candidate: 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:09:07,290 - metrics_logger - INFO - Getting embeddings for the sentence: What is the RAG-Sequnce model and how does it work?
2024-12-05 11:09:09,418 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.
2024-12-05 11:09:20,772 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:09:20,773 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What is the RAG-Sequnce model and how does it work?' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:09:20,773 - metrics_logger - INFO - Calculating Coverage Score between 'What is the RAG-Sequnce model and how does it work?' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:09:20,773 - metrics_logger - INFO - Calculating Relevance Score between 'What is the RAG-Sequnce model and how does it work?' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:09:24,330 - metrics_logger - INFO - Calculating all metrics for reference: 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and candidate: 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:09:24,331 - metrics_logger - INFO - Getting embeddings for the sentence: As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?
2024-12-05 11:09:26,462 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.
2024-12-05 11:09:38,399 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:09:38,400 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:09:38,400 - metrics_logger - INFO - Calculating Coverage Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:09:38,400 - metrics_logger - INFO - Calculating Relevance Score between 'As a Research Scientist in Machine Learning focusing on hyperspectral imaging, how does the use of Wikipedia as a non-parametric knowledge source enhance the performance of RAG models in open-domain question answering tasks?' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:09:41,955 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht is the perfomance of T5 in open-domain QA tasks?' and candidate: 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:09:41,956 - metrics_logger - INFO - Getting embeddings for the sentence: Wht is the perfomance of T5 in open-domain QA tasks?
2024-12-05 11:09:44,052 - metrics_logger - INFO - Getting embeddings for the sentence: The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.
2024-12-05 11:09:56,002 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:09:56,003 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:09:56,003 - metrics_logger - INFO - Calculating Coverage Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:09:56,003 - metrics_logger - INFO - Calculating Relevance Score between 'Wht is the perfomance of T5 in open-domain QA tasks?' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:09:59,555 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual tracking?' and candidate: 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:09:59,556 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual tracking?
2024-12-05 11:10:01,642 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.
2024-12-05 11:10:14,404 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:10:14,404 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual tracking?' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:10:14,405 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual tracking?' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:10:14,405 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual tracking?' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:10:17,870 - metrics_logger - INFO - Calculating all metrics for reference: 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and candidate: 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:10:17,871 - metrics_logger - INFO - Getting embeddings for the sentence: What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?
2024-12-05 11:10:19,955 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.
2024-12-05 11:10:32,090 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:10:32,091 - metrics_logger - INFO - Calculating Jaccard Similarity between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:10:32,092 - metrics_logger - INFO - Calculating Coverage Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:10:32,092 - metrics_logger - INFO - Calculating Relevance Score between 'What advancements does the SAMURAI model bring to Visual Object Tracking (VOT) compared to previous models, and how does it utilize motion-aware memory selection to enhance tracking performance?' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:10:35,751 - metrics_logger - INFO - Calculating all metrics for reference: 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and candidate: 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:10:35,751 - metrics_logger - INFO - Getting embeddings for the sentence: As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?
2024-12-05 11:10:38,137 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.
2024-12-05 11:10:49,366 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:10:49,367 - metrics_logger - INFO - Calculating Jaccard Similarity between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:10:49,367 - metrics_logger - INFO - Calculating Coverage Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:10:49,367 - metrics_logger - INFO - Calculating Relevance Score between 'As a research scientist focused on developing advanced algorithms for hyperspectral imaging, how does the SAMURAI model enhance visual object tracking in complex scenarios, particularly in relation to its motion-aware memory selection mechanism and its performance compared to existing trackers?' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:10:53,095 - metrics_logger - INFO - Calculating all metrics for reference: 'how samurai help in visual object tracking?' and candidate: 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:10:53,096 - metrics_logger - INFO - Getting embeddings for the sentence: how samurai help in visual object tracking?
2024-12-05 11:10:55,172 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how SAMURAI helps in visual object tracking.
2024-12-05 11:11:07,269 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:11:07,270 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how samurai help in visual object tracking?' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:11:07,270 - metrics_logger - INFO - Calculating Coverage Score between 'how samurai help in visual object tracking?' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:11:07,271 - metrics_logger - INFO - Calculating Relevance Score between 'how samurai help in visual object tracking?' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:11:10,776 - metrics_logger - INFO - Calculating all metrics for reference: 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and candidate: 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:11:10,777 - metrics_logger - INFO - Getting embeddings for the sentence: How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?
2024-12-05 11:11:12,808 - metrics_logger - INFO - Getting embeddings for the sentence: The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.
2024-12-05 11:11:24,504 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:11:24,504 - metrics_logger - INFO - Calculating Jaccard Similarity between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:11:24,505 - metrics_logger - INFO - Calculating Coverage Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:11:24,505 - metrics_logger - INFO - Calculating Relevance Score between 'How does the DPR retriever enhance the performance of the RAG model in open-domain question answering?' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:11:28,265 - metrics_logger - INFO - Calculating all metrics for reference: 'how RAG-Sequence model work with BERT and what it do for image classification?' and candidate: 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:11:28,266 - metrics_logger - INFO - Getting embeddings for the sentence: how RAG-Sequence model work with BERT and what it do for image classification?
2024-12-05 11:11:30,330 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.
2024-12-05 11:11:43,883 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:11:43,883 - metrics_logger - INFO - Calculating Jaccard Similarity between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:11:43,884 - metrics_logger - INFO - Calculating Coverage Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:11:43,884 - metrics_logger - INFO - Calculating Relevance Score between 'how RAG-Sequence model work with BERT and what it do for image classification?' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:11:47,524 - metrics_logger - INFO - Calculating all metrics for reference: 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and candidate: 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:11:47,525 - metrics_logger - INFO - Getting embeddings for the sentence: Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?
2024-12-05 11:11:50,083 - metrics_logger - INFO - Getting embeddings for the sentence: The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.
2024-12-05 11:12:02,372 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:12:02,372 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:12:02,373 - metrics_logger - INFO - Calculating Coverage Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:12:02,373 - metrics_logger - INFO - Calculating Relevance Score between 'Wht are the main benfits of BayLing 2 in terms of multilingual capabilitis?' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:12:05,874 - metrics_logger - INFO - Calculating all metrics for reference: 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and candidate: 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:12:05,874 - metrics_logger - INFO - Getting embeddings for the sentence: Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?
2024-12-05 11:12:08,003 - metrics_logger - INFO - Getting embeddings for the sentence: The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.
2024-12-05 11:12:20,052 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:12:20,053 - metrics_logger - INFO - Calculating Jaccard Similarity between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:12:20,053 - metrics_logger - INFO - Calculating Coverage Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:12:20,054 - metrics_logger - INFO - Calculating Relevance Score between 'Can you explain how BayLing 2 enhances multilingual capabilities in LLMs, especially in relation to low-resource languages, and how it compares to models like GPT-3 and GPT-4?' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:12:23,907 - metrics_logger - INFO - Calculating metrics for pairs: 'reference' and 'candidate'
2024-12-05 11:12:23,908 - metrics_logger - INFO - Calculating all metrics for reference: 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and candidate: 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:12:23,909 - metrics_logger - INFO - Getting embeddings for the sentence: The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.
2024-12-05 11:12:26,072 - metrics_logger - INFO - Getting embeddings for the sentence: The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.
2024-12-05 11:12:37,842 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:12:37,843 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:12:37,843 - metrics_logger - INFO - Calculating Coverage Score between 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:12:37,843 - metrics_logger - INFO - Calculating Relevance Score between 'The HuggingFace Transformers Library provides open-sourced code to run experiments with RAG models, which utilize input sequences to retrieve text documents and use them as additional context for generating target sequences. This library allows researchers to access the necessary tools and resources to implement and experiment with RAG models effectively, including the retriever and generator components that are essential for training these models end-to-end.' and 'The provided document does not contain specific information on how the HuggingFace Transformers Library facilitates the implementation of RAG models in machine learning research.'
2024-12-05 11:12:41,483 - metrics_logger - INFO - Calculating all metrics for reference: 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and candidate: 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:12:41,484 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.
2024-12-05 11:12:44,101 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.
2024-12-05 11:12:56,395 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:12:56,395 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:12:56,396 - metrics_logger - INFO - Calculating Coverage Score between 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:12:56,396 - metrics_logger - INFO - Calculating Relevance Score between 'The RAG-Sequence model uses the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized. Formally, this is expressed as pRAG-Sequence(y|x) â‰ˆ (cid:88) pÎ·(z|x)pÎ¸(y|x, z) = (cid:88) pÎ·(z|x) N (cid:89) pÎ¸(yi|x, z, y1:iâˆ’1) where z is in the top-k documents. The model can also be used for sequence classification tasks by considering the target class as a target sequence of length one, making RAG-Sequence and RAG-Token equivalent in this context.' and 'The RAG (Retrieval-Augmented Generation) Sequence model is a method that uses an input sequence to retrieve text documents and use them as additional context when generating the target sequence. It consists of two components: a retriever and a generator. The retriever, with parameters Î·, returns distributions over text passages given a query. The generator, parameterized by Î¸, generates the target sequence based on the input sequence, retrieved text, and previously generated sequences.'
2024-12-05 11:13:00,530 - metrics_logger - INFO - Calculating all metrics for reference: 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and candidate: 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:13:00,531 - metrics_logger - INFO - Getting embeddings for the sentence: In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.
2024-12-05 11:13:03,120 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.
2024-12-05 11:13:15,593 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:13:15,593 - metrics_logger - INFO - Calculating Jaccard Similarity between 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:13:15,594 - metrics_logger - INFO - Calculating Coverage Score between 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:13:15,594 - metrics_logger - INFO - Calculating Relevance Score between 'In the context of RAG models, Wikipedia serves as a crucial non-parametric knowledge source by providing a vast repository of information that can be utilized for open-domain question answering (QA). The experiments conducted with RAG utilize a single Wikipedia dump, specifically the December 2018 version, which is split into 100-word chunks, resulting in a total of 21 million documents. This extensive dataset allows the document encoder to compute embeddings for each document, facilitating the retrieval of relevant information during the QA process. By retrieving the top k documents for each query, RAG can effectively minimize the negative log-likelihood of answers, thereby enhancing the accuracy of responses. The comparison of RAG to traditional extractive QA paradigms highlights its ability to generate answers rather than merely extracting spans from retrieved documents, which is particularly beneficial in knowledge-intensive tasks. Furthermore, RAG's capability to leverage both non-parametric knowledge from Wikipedia and parametric knowledge allows it to generate reasonable responses even for questions that cannot be answered solely using Wikipedia, thus improving overall performance in open-domain QA tasks.' and 'The document does not provide specific information on how the use of Wikipedia as a non-parametric knowledge source enhances the performance of RAG models in open-domain question answering tasks, especially in the context of hyperspectral imaging.'
2024-12-05 11:13:19,498 - metrics_logger - INFO - Calculating all metrics for reference: 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and candidate: 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:13:19,499 - metrics_logger - INFO - Getting embeddings for the sentence: On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.
2024-12-05 11:13:21,672 - metrics_logger - INFO - Getting embeddings for the sentence: The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.
2024-12-05 11:13:32,768 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:13:32,769 - metrics_logger - INFO - Calculating Jaccard Similarity between 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:13:32,769 - metrics_logger - INFO - Calculating Coverage Score between 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:13:32,769 - metrics_logger - INFO - Calculating Relevance Score between 'On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). The T5-11B model achieved a score of 34.5 on the NQ task and 36.6 on the TQA task, demonstrating its competitive performance in comparison to other models.' and 'The performance of T5 in open-domain QA tasks is as follows: For Natural Questions (NQ), T5-11B scored 34.5 and T5-11B+SSM scored 36.6. For TriviaQA (TQA), T5-11B scored 50.1 and T5-11B+SSM scored 60.5. However, these scores are for the standard test set for Open-Domain QA. The document does not provide scores for T5 on the TQA-Wiki test set.'
2024-12-05 11:13:36,485 - metrics_logger - INFO - Calculating all metrics for reference: 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and candidate: 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:13:36,486 - metrics_logger - INFO - Getting embeddings for the sentence: SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.
2024-12-05 11:13:38,822 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.
2024-12-05 11:13:50,607 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:13:50,608 - metrics_logger - INFO - Calculating Jaccard Similarity between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:13:50,608 - metrics_logger - INFO - Calculating Coverage Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:13:50,609 - metrics_logger - INFO - Calculating Relevance Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across various benchmark datasets, achieving significant improvements in success rate and precision over existing trackers.' and 'The SAMURAI visual object tracker strengthens the model's ability to track objects accurately in complex video scenarios.'
2024-12-05 11:13:54,211 - metrics_logger - INFO - Calculating all metrics for reference: 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and candidate: 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:13:54,212 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.
2024-12-05 11:13:56,803 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.
2024-12-05 11:14:08,603 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:14:08,604 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:14:08,604 - metrics_logger - INFO - Calculating Coverage Score between 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:14:08,604 - metrics_logger - INFO - Calculating Relevance Score between 'The SAMURAI model introduces significant advancements in Visual Object Tracking (VOT) by enhancing the Segment Anything Model 2 (SAM 2) specifically for tracking applications. One of the primary improvements is the incorporation of a motion-aware memory selection mechanism, which addresses the challenges faced by SAM 2 in managing crowded scenes with fast-moving or self-occluding objects. This mechanism allows SAMURAI to effectively predict object motion and refine mask selection, leading to robust and accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving notable improvements in success rate and precision over existing trackers. For instance, it shows a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k, indicating its robustness in complex tracking scenarios. Additionally, SAMURAI's ability to generalize without fine-tuning positions it competitively against fully supervised methods, showcasing its potential for real-world applications in dynamic environments.' and 'The SAMURAI model brings two key advancements to Visual Object Tracking (VOT) compared to previous models. First, it incorporates a motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios. Second, it introduces an optimized memory selection mechanism. 

SAMURAI utilizes motion-aware memory selection to enhance tracking performance by leveraging the history of object trajectories. This helps the model differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by avoiding the indiscriminate storage of recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. 

This information comes from the provided document.'
2024-12-05 11:14:13,486 - metrics_logger - INFO - Calculating all metrics for reference: 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and candidate: 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:14:13,487 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.
2024-12-05 11:14:16,129 - metrics_logger - INFO - Getting embeddings for the sentence: The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.
2024-12-05 11:14:27,755 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:14:27,755 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:14:27,756 - metrics_logger - INFO - Calculating Coverage Score between 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:14:27,756 - metrics_logger - INFO - Calculating Relevance Score between 'The SAMURAI model enhances visual object tracking (VOT) by incorporating a motion-aware memory selection mechanism that effectively predicts object motion and refines mask selection. This adaptation of the Segment Anything Model 2 (SAM 2) addresses challenges faced in VOT scenarios, such as managing crowded scenes with fast-moving or self-occluding objects. The original SAM 2 model's fixed-window memory approach did not consider the quality of memories selected for conditioning image features for subsequent frames, which led to error propagation in videos. SAMURAI overcomes this limitation by integrating temporal motion cues, allowing it to select the most relevant memories based on their motion characteristics. This results in robust and accurate tracking without the need for retraining or fine-tuning. In evaluations, SAMURAI demonstrates significant improvements in success rate and precision over existing trackers, achieving a 7.1% AUC gain on LaSOText and a 3.5% AO gain on GOT-10k. Furthermore, it shows competitive results compared to fully supervised methods on La-SOT, highlighting its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.' and 'The SAMURAI model enhances visual object tracking in complex scenarios by incorporating motion information into the prediction process. This allows the model to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. The model also optimizes memory management by not indiscriminately storing recent frames in the memory bank, which can introduce irrelevant features during occlusions and compromise tracking performance. These advancements allow the SAMURAI model to adapt rich mask information for robust video object tracking. However, the document does not provide a direct comparison of the SAMURAI model's performance with existing trackers.'
2024-12-05 11:14:31,924 - metrics_logger - INFO - Calculating all metrics for reference: 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and candidate: 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:14:31,925 - metrics_logger - INFO - Getting embeddings for the sentence: SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.
2024-12-05 11:14:34,266 - metrics_logger - INFO - Getting embeddings for the sentence: The document does not provide specific information on how SAMURAI helps in visual object tracking.
2024-12-05 11:14:45,469 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:14:45,469 - metrics_logger - INFO - Calculating Jaccard Similarity between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:14:45,470 - metrics_logger - INFO - Calculating Coverage Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:14:45,470 - metrics_logger - INFO - Calculating Relevance Score between 'SAMURAI is an enhanced adaptation of the Segment Anything Model 2 (SAM 2) specifically designed for visual object tracking (VOT). It incorporates temporal motion cues and a motion-aware memory selection mechanism, which allows it to effectively predict object motion and refine mask selection. This results in robust and accurate tracking, particularly in challenging scenarios like crowded scenes with fast-moving or self-occluding objects. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, achieving significant improvements in success rate and precision over existing trackers. Importantly, it does not require retraining or fine-tuning, making it a reliable solution for online VOT.' and 'The document does not provide specific information on how SAMURAI helps in visual object tracking.'
2024-12-05 11:14:48,978 - metrics_logger - INFO - Calculating all metrics for reference: 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and candidate: 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:14:48,979 - metrics_logger - INFO - Getting embeddings for the sentence: The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.
2024-12-05 11:14:51,333 - metrics_logger - INFO - Getting embeddings for the sentence: The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.
2024-12-05 11:15:03,117 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:15:03,117 - metrics_logger - INFO - Calculating Jaccard Similarity between 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:15:03,118 - metrics_logger - INFO - Calculating Coverage Score between 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:15:03,118 - metrics_logger - INFO - Calculating Relevance Score between 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by utilizing a bi-encoder architecture that produces dense representations of documents and queries using BERT. This allows the retrieval of the top K documents that are most relevant to a given query, which are then used by the generator to produce answers. The DPR retriever was specifically trained on datasets like Natural Questions and TriviaQA, enabling it to effectively retrieve documents that contain answers to questions. This retrieval mechanism is crucial as it allows RAG to combine the generation flexibility of closed-book approaches with the performance of open-book retrieval-based methods, achieving state-of-the-art results without the need for complex re-ranking or extractive reading systems.' and 'The DPR retriever enhances the performance of the RAG model in open-domain question answering by providing retrieval supervision on Natural Questions and TriviaQA. The RAG model uses this to compare favorably to the DPR QA system, which uses a BERT-based "cross-encoder" to re-rank documents, along with an extractive reader. The RAG model demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. This information comes from the retrieved document.'
2024-12-05 11:15:07,062 - metrics_logger - INFO - Calculating all metrics for reference: 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and candidate: 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:15:07,064 - metrics_logger - INFO - Getting embeddings for the sentence: RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.
2024-12-05 11:15:09,742 - metrics_logger - INFO - Getting embeddings for the sentence: The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.
2024-12-05 11:15:22,667 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:15:22,668 - metrics_logger - INFO - Calculating Jaccard Similarity between 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:15:22,668 - metrics_logger - INFO - Calculating Coverage Score between 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:15:22,668 - metrics_logger - INFO - Calculating Relevance Score between 'RAG-Sequence model uses retrieved documents to generate a complete sequence by treating the retrieved document as a single latent variable. It calculates the sequence probability p(y|x) using a top-K approximation. The model retrieves the top K documents and generates output sequence probabilities for each document, which are then marginalized. The retrieval component is based on a bi-encoder architecture using BERT, where a dense representation of a document and a query representation are produced by BERT encoders. This allows the model to effectively retrieve relevant documents that can enhance the accuracy and efficiency of tasks, including those related to image classification in hyperspectral imaging.' and 'The RAG-Sequence model works with BERT by using the same retrieved document to generate the complete sequence. It treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability via a top-K approximation. The top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.

The RAG-Token model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token.

The retrieval component is based on DPR, which follows a bi-encoder architecture. It uses a BERTBASE document encoder to produce a dense representation of a document, and a query encoder, also based on BERTBASE, to produce a query representation.

However, the document does not provide information on how RAG-Sequence model works with BERT for image classification.'
2024-12-05 11:15:28,108 - metrics_logger - INFO - Calculating all metrics for reference: 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and candidate: 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:15:28,109 - metrics_logger - INFO - Getting embeddings for the sentence: BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.
2024-12-05 11:15:30,322 - metrics_logger - INFO - Getting embeddings for the sentence: The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.
2024-12-05 11:15:42,063 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:15:42,064 - metrics_logger - INFO - Calculating Jaccard Similarity between 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:15:42,064 - metrics_logger - INFO - Calculating Coverage Score between 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:15:42,064 - metrics_logger - INFO - Calculating Relevance Score between 'BayLing 2 enhances LLMâ€™s multilingual capabilities through language alignment, focusing on transferring knowledge and generative abilities from high-resource languages to low-resource languages. It achieves outstanding translation performance across over 100 languages and demonstrates superior multilingual knowledge and understanding capability. Additionally, it maintains robust proficiency in high-resource languages like Chinese and English, making it an efficient solution for multilingual interaction.' and 'The main benefits of BayLing 2 in terms of multilingual capabilities are its superior multilingual translation capabilities on Flores-101 and WMT22, making it a leading tool in the field of multilingual translation. It offers significant advancements in multilingual capabilities of Large Language Models (LLMs). BayLing's multilingual performance was assessed using several benchmarks through the Language Model Evaluation Harness, an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. The results were obtained in a zero-shot setting.'
2024-12-05 11:15:46,108 - metrics_logger - INFO - Calculating all metrics for reference: 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and candidate: 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:15:46,110 - metrics_logger - INFO - Getting embeddings for the sentence: BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.
2024-12-05 11:15:48,890 - metrics_logger - INFO - Getting embeddings for the sentence: The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.
2024-12-05 11:16:01,115 - metrics_logger - INFO - Calculating Cosine Similarity between embeddings
2024-12-05 11:16:01,116 - metrics_logger - INFO - Calculating Jaccard Similarity between 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:16:01,116 - metrics_logger - INFO - Calculating Coverage Score between 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:16:01,116 - metrics_logger - INFO - Calculating Relevance Score between 'BayLing 2 enhances multilingual capabilities in LLMs primarily through its efficient language alignment and the use of high-resource languages as pivot languages. It applies cross-lingual tasks to align over 100 languages to these pivot languages, facilitating the transfer of capabilities from high-resource to low-resource languages. This approach addresses the issue of language imbalance, where high-resource languages dominate the training data, leading to performance variations across different languages. BayLing 2's instruction dataset includes both high-resource languages and cross-lingual instructions, which are designed to activate the LLMs' instruction-following capabilities and facilitate multilingual alignment.

In terms of performance, BayLing models have shown superior multilingual translation capabilities compared to foundational models like GPT-3 and GPT-4. For instance, in evaluations on the WMT22 benchmark, BayLing achieved the highest overall translation performance among open-sourced models, coming close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. Specifically, BayLing's improved language alignment allows it to produce more accurate translations across different languages, particularly benefiting low-resource languages. This is crucial as foundational models often struggle with off-target issues when generating outputs in these languages. Overall, BayLing's approach not only enhances multilingual generation capabilities but also demonstrates significant improvements in translation performance, especially for low-resource linguistic communities.' and 'The document provided does not give specific details on how BayLing 2 enhances multilingual capabilities in Language Learning Models (LLMs), especially in relation to low-resource languages. It also does not provide a direct comparison between BayLing 2 and models like GPT-3 and GPT-4. However, it does mention that BayLing can improve the multilingual generation abilities of LLMs through cross-lingual translation data, without the need for extensive multilingual instruction data. This is considered crucial for efficiently enhancing the multilingual capabilities of LLMs.'
2024-12-05 11:16:05,374 - metrics_logger - INFO - Metrics saved successfully to 'output_metrics_parallel.xlsx'
2024-12-05 11:16:05,374 - metrics_logger - INFO - Metrics saved successfully to 'output_metrics.xlsx'
